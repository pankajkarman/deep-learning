{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autotime loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from init import *\n",
    "from music21 import converter, stream, instrument, note, chord\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation\n",
    "from keras.layers import BatchNormalization as BatchNorm\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 24.8 ms\n"
     ]
    }
   ],
   "source": [
    "class Music():\n",
    "    def __init__(self, filenames):\n",
    "        self.files = filenames\n",
    "        \n",
    "    def get_musical_notes(self):\n",
    "        notes = []\n",
    "        for filename in self.files:\n",
    "            song = converter.parse(filename)\n",
    "            parts = instrument.partitionByInstrument(song)\n",
    "            notes_to_parse = None\n",
    "            if parts:\n",
    "                notes_to_parse = parts.parts[0].recurse()\n",
    "            else:\n",
    "                notes_to_parse = song.flat.notes\n",
    "                \n",
    "            for el in notes_to_parse:\n",
    "                if isinstance(el, note.Note):\n",
    "                    notes.append(str(el.pitch))\n",
    "                elif isinstance(el, chord.Chord):\n",
    "                    chords = '.'.join(str(n) for n in el.normalOrder)\n",
    "                    notes.append(chords)\n",
    "        return notes\n",
    "    \n",
    "    def get_encoded_notes(self):\n",
    "        notes = self.get_musical_notes()\n",
    "        self.unique_notes = sorted(set(notes))\n",
    "        self.pitchdict = {note:number for number, note in enumerate(self.unique_notes)}\n",
    "        self.inverse_pitchdict = {number:note for number, note in enumerate(self.unique_notes)}\n",
    "        return list(map(self.pitchdict.get, notes))\n",
    "    \n",
    "    def get_training_sequences(self, sequence_length=100):\n",
    "        ins = []\n",
    "        out = []\n",
    "        notes = self.get_encoded_notes()\n",
    "        for i in range(0, len(notes) - sequence_length, 1):\n",
    "                ins.append(notes[i:i+sequence_length])\n",
    "                out.append(notes[i+sequence_length])\n",
    "        ins = np.expand_dims(np.array(ins), axis=2) / float(len(self.unique_notes))\n",
    "        out = to_categorical(out)\n",
    "        return ins, out\n",
    "    \n",
    "    def compile_model(self, inputs, latent_dim=256):\n",
    "        shape = (inputs.shape[1], inputs.shape[2])\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(latent_dim, input_shape=shape, recurrent_dropout=0.3, return_sequences=True))\n",
    "        model.add(LSTM(2 * latent_dim, recurrent_dropout=0.3, return_sequences=True))\n",
    "        model.add(LSTM(latent_dim))\n",
    "        model.add(BatchNorm())\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(latent_dim))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNorm())\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(len(self.unique_notes)))\n",
    "        model.add(Activation('softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "        self.model = model\n",
    "        return model\n",
    "    \n",
    "    def train(self, X, y, checkpoint='best_model.h5', epochs=100, batch_size=128, split=0.01, verbose=0):\n",
    "        mc = ModelCheckpoint(checkpoint, monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "        self.history = self.model.fit(X, y, epochs=epochs, batch_size=batch_size, \n",
    "                                      validation_split=split, verbose=verbose, \n",
    "                                      callbacks=[mc])\n",
    "        return self.history\n",
    "    \n",
    "    def predict(self, sequence, model_name='best_model.h5'):\n",
    "        sequence = list(np.squeeze(sequence))      \n",
    "        model = load_model(model_name)\n",
    "        output = []\n",
    "        for note_index in range(500):\n",
    "            inputs = np.reshape(sequence, (1, len(sequence), 1)) \n",
    "            inputs = inputs / float(len(self.unique_notes))\n",
    "            prediction = model.predict(inputs, verbose=0)\n",
    "            index = np.argmax(prediction)\n",
    "            output.append(self.inverse_pitchdict[index])\n",
    "            sequence.append(index)\n",
    "            sequence = sequence[1:len(sequence)]\n",
    "        return output\n",
    "    \n",
    "    def generate_notes(self, sequence, model_name='best_model.h5', outfile='test_output.mid'):\n",
    "        offset = 0\n",
    "        output_notes = []\n",
    "        sequence = list(np.squeeze(sequence))  \n",
    "        out_notes = self.predict(sequence, model_name)\n",
    "        for pattern in out_notes:\n",
    "            if ('.' in pattern) or pattern.isdigit():\n",
    "                notes_in_chord = pattern.split('.')\n",
    "                notes = []\n",
    "                for current_note in notes_in_chord:\n",
    "                    new_note = note.Note(int(current_note))\n",
    "                    new_note.storedInstrument = instrument.Piano()\n",
    "                    notes.append(new_note)\n",
    "                new = chord.Chord(notes)\n",
    "            else:\n",
    "                new = note.Note(pattern)\n",
    "                new.storedInstrument = instrument.Piano()        \n",
    "            new.offset = offset\n",
    "            output_notes.append(new)\n",
    "            offset += 0.5\n",
    "        midi_stream = stream.Stream(output_notes)\n",
    "        midi_stream.write('midi', fp=outfile)\n",
    "        print('New music has been generated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100, 256)          264192    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100, 512)          1574912   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               787456    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 358)               92006     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 358)               0         \n",
      "=================================================================\n",
      "Total params: 2,786,406\n",
      "Trainable params: 2,785,382\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "None\n",
      "time: 46.5 s\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 256\n",
    "latent_dim = 256\n",
    "sequence_length = 100\n",
    "checkpoint = 'best_model.h5'\n",
    "files = glob.glob('./songs/*.mid')\n",
    "music = Music(files)\n",
    "notes = music.get_encoded_notes()\n",
    "ins, out = music.get_training_sequences(sequence_length)\n",
    "model = music.compile_model(ins, latent_dim)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 56506 samples, validate on 571 samples\n",
      "Epoch 1/50\n",
      "56506/56506 [==============================] - 1889s 33ms/step - loss: 5.2698 - val_loss: 4.3882\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.38825, saving model to best_model.h5\n",
      "Epoch 2/50\n",
      "56506/56506 [==============================] - 1793s 32ms/step - loss: 4.7342 - val_loss: 4.6034\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 4.38825\n",
      "Epoch 3/50\n",
      "56506/56506 [==============================] - 1774s 31ms/step - loss: 4.6316 - val_loss: 5.2696\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 4.38825\n",
      "Epoch 4/50\n",
      "56506/56506 [==============================] - 1772s 31ms/step - loss: 4.5786 - val_loss: 4.3765\n",
      "\n",
      "Epoch 00004: val_loss improved from 4.38825 to 4.37649, saving model to best_model.h5\n",
      "Epoch 5/50\n",
      "56506/56506 [==============================] - 1774s 31ms/step - loss: 4.5407 - val_loss: 4.5136\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 4.37649\n",
      "Epoch 6/50\n",
      "56506/56506 [==============================] - 1773s 31ms/step - loss: 4.4974 - val_loss: 4.2581\n",
      "\n",
      "Epoch 00006: val_loss improved from 4.37649 to 4.25812, saving model to best_model.h5\n",
      "Epoch 7/50\n",
      "56506/56506 [==============================] - 1774s 31ms/step - loss: 4.4495 - val_loss: 4.2867\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 4.25812\n",
      "Epoch 8/50\n",
      "56506/56506 [==============================] - 1773s 31ms/step - loss: 4.3933 - val_loss: 4.2762\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 4.25812\n",
      "Epoch 9/50\n",
      "56506/56506 [==============================] - 1773s 31ms/step - loss: 4.3343 - val_loss: 4.1925\n",
      "\n",
      "Epoch 00009: val_loss improved from 4.25812 to 4.19251, saving model to best_model.h5\n",
      "Epoch 10/50\n",
      "56506/56506 [==============================] - 1773s 31ms/step - loss: 4.2513 - val_loss: 4.2202\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 4.19251\n",
      "Epoch 11/50\n",
      "56506/56506 [==============================] - 2059s 36ms/step - loss: 4.1787 - val_loss: 4.3385\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 4.19251\n",
      "Epoch 12/50\n",
      "56506/56506 [==============================] - 2022s 36ms/step - loss: 4.1035 - val_loss: 4.1916\n",
      "\n",
      "Epoch 00012: val_loss improved from 4.19251 to 4.19157, saving model to best_model.h5\n",
      "Epoch 13/50\n",
      "56506/56506 [==============================] - 1787s 32ms/step - loss: 4.0352 - val_loss: 4.2878\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 4.19157\n",
      "Epoch 14/50\n",
      "56506/56506 [==============================] - 1758s 31ms/step - loss: 3.9649 - val_loss: 4.4575\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 4.19157\n",
      "Epoch 15/50\n",
      "56506/56506 [==============================] - 1812s 32ms/step - loss: 3.8877 - val_loss: 4.3732\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 4.19157\n",
      "Epoch 16/50\n",
      "56506/56506 [==============================] - 1723s 30ms/step - loss: 3.8152 - val_loss: 4.1910\n",
      "\n",
      "Epoch 00016: val_loss improved from 4.19157 to 4.19102, saving model to best_model.h5\n",
      "Epoch 17/50\n",
      "56506/56506 [==============================] - 1718s 30ms/step - loss: 3.7402 - val_loss: 4.2139\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 4.19102\n",
      "Epoch 18/50\n",
      "56506/56506 [==============================] - 1738s 31ms/step - loss: 3.6699 - val_loss: 4.2163\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 4.19102\n",
      "Epoch 19/50\n",
      "56506/56506 [==============================] - 1993s 35ms/step - loss: 3.5995 - val_loss: 4.3170\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 4.19102\n",
      "Epoch 20/50\n",
      "56506/56506 [==============================] - 2030s 36ms/step - loss: 3.5212 - val_loss: 4.2593\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 4.19102\n",
      "Epoch 21/50\n",
      "56506/56506 [==============================] - 2038s 36ms/step - loss: 3.4483 - val_loss: 4.1967\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 4.19102\n",
      "Epoch 22/50\n",
      "56506/56506 [==============================] - 1833s 32ms/step - loss: 3.3925 - val_loss: 4.2589\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 4.19102\n",
      "Epoch 23/50\n",
      "56506/56506 [==============================] - 1740s 31ms/step - loss: 3.3206 - val_loss: 4.2421\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 4.19102\n",
      "Epoch 24/50\n",
      "56506/56506 [==============================] - 1787s 32ms/step - loss: 3.2613 - val_loss: 4.2361\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 4.19102\n",
      "Epoch 25/50\n",
      "56506/56506 [==============================] - 1839s 33ms/step - loss: 3.1977 - val_loss: 4.2538\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 4.19102\n",
      "Epoch 26/50\n",
      "56506/56506 [==============================] - 1859s 33ms/step - loss: 3.1385 - val_loss: 4.1940\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 4.19102\n",
      "Epoch 27/50\n",
      "56506/56506 [==============================] - 1835s 32ms/step - loss: 3.0764 - val_loss: 4.3478\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 4.19102\n",
      "Epoch 28/50\n",
      "56506/56506 [==============================] - 1760s 31ms/step - loss: 3.0277 - val_loss: 4.2225\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 4.19102\n",
      "Epoch 29/50\n",
      "56506/56506 [==============================] - 1766s 31ms/step - loss: 2.9729 - val_loss: 4.2003\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 4.19102\n",
      "Epoch 30/50\n",
      "56506/56506 [==============================] - 1770s 31ms/step - loss: 2.9258 - val_loss: 4.3118\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 4.19102\n",
      "Epoch 31/50\n",
      "56506/56506 [==============================] - 1770s 31ms/step - loss: 2.8661 - val_loss: 4.3219\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 4.19102\n",
      "Epoch 32/50\n",
      "56506/56506 [==============================] - 1773s 31ms/step - loss: 2.8214 - val_loss: 4.1383\n",
      "\n",
      "Epoch 00032: val_loss improved from 4.19102 to 4.13833, saving model to best_model.h5\n",
      "Epoch 33/50\n",
      "56506/56506 [==============================] - 1772s 31ms/step - loss: 2.7728 - val_loss: 4.3018\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 4.13833\n",
      "Epoch 34/50\n",
      "56506/56506 [==============================] - 1771s 31ms/step - loss: 2.7282 - val_loss: 4.2857\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 4.13833\n",
      "Epoch 35/50\n",
      "56506/56506 [==============================] - 1770s 31ms/step - loss: 2.6809 - val_loss: 4.2903\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 4.13833\n",
      "Epoch 36/50\n",
      "56506/56506 [==============================] - 1769s 31ms/step - loss: 2.6381 - val_loss: 4.2977\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 4.13833\n",
      "Epoch 37/50\n",
      "56506/56506 [==============================] - 1769s 31ms/step - loss: 2.5868 - val_loss: 4.4922\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 4.13833\n",
      "Epoch 38/50\n",
      "56506/56506 [==============================] - 1769s 31ms/step - loss: 2.5521 - val_loss: 4.2662\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 4.13833\n",
      "Epoch 39/50\n",
      "56506/56506 [==============================] - 1766s 31ms/step - loss: 2.5154 - val_loss: 4.3109\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 4.13833\n",
      "Epoch 40/50\n",
      "56506/56506 [==============================] - 1769s 31ms/step - loss: 2.4711 - val_loss: 4.3558\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 4.13833\n",
      "Epoch 41/50\n",
      "56506/56506 [==============================] - 1777s 31ms/step - loss: 2.4394 - val_loss: 4.4056\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 4.13833\n",
      "Epoch 42/50\n",
      "56506/56506 [==============================] - 1943s 34ms/step - loss: 2.3976 - val_loss: 4.2265\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 4.13833\n",
      "Epoch 43/50\n",
      "56506/56506 [==============================] - 1886s 33ms/step - loss: 2.3591 - val_loss: 4.4564\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 4.13833\n",
      "Epoch 44/50\n",
      "56506/56506 [==============================] - 1774s 31ms/step - loss: 2.3250 - val_loss: 4.3547\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 4.13833\n",
      "Epoch 45/50\n",
      "56506/56506 [==============================] - 1764s 31ms/step - loss: 2.2901 - val_loss: 4.3717\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 4.13833\n",
      "Epoch 46/50\n",
      "56506/56506 [==============================] - 1763s 31ms/step - loss: 2.2517 - val_loss: 4.3212\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 4.13833\n",
      "Epoch 47/50\n",
      "56506/56506 [==============================] - 1761s 31ms/step - loss: 2.2139 - val_loss: 4.4224\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 4.13833\n",
      "Epoch 48/50\n",
      "56506/56506 [==============================] - 1773s 31ms/step - loss: 2.1805 - val_loss: 4.3055\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 4.13833\n",
      "Epoch 49/50\n",
      "56506/56506 [==============================] - 1778s 31ms/step - loss: 2.1521 - val_loss: 4.3566\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 4.13833\n",
      "Epoch 50/50\n",
      "56506/56506 [==============================] - 1792s 32ms/step - loss: 2.1182 - val_loss: 4.3151\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 4.13833\n",
      "time: 1d 1h 7min 26s\n"
     ]
    }
   ],
   "source": [
    "hist = music.train(ins, out, checkpoint, epochs, batch_size, verbose=1, split=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New music has been generated.\n",
      "time: 58.4 s\n"
     ]
    }
   ],
   "source": [
    "music.generate_notes(ins[2], outfile='new_music.mid')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
