{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autotime loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from init import *\n",
    "from music21 import converter, stream, instrument, note, chord\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation\n",
    "from keras.layers import BatchNormalization as BatchNorm\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 24.8 ms\n"
     ]
    }
   ],
   "source": [
    "class Music():\n",
    "    def __init__(self, filenames):\n",
    "        self.files = filenames\n",
    "        \n",
    "    def get_musical_notes(self):\n",
    "        notes = []\n",
    "        for filename in self.files:\n",
    "            song = converter.parse(filename)\n",
    "            parts = instrument.partitionByInstrument(song)\n",
    "            notes_to_parse = None\n",
    "            if parts:\n",
    "                notes_to_parse = parts.parts[0].recurse()\n",
    "            else:\n",
    "                notes_to_parse = song.flat.notes\n",
    "                \n",
    "            for el in notes_to_parse:\n",
    "                if isinstance(el, note.Note):\n",
    "                    notes.append(str(el.pitch))\n",
    "                elif isinstance(el, chord.Chord):\n",
    "                    chords = '.'.join(str(n) for n in el.normalOrder)\n",
    "                    notes.append(chords)\n",
    "        return notes\n",
    "    \n",
    "    def get_encoded_notes(self):\n",
    "        notes = self.get_musical_notes()\n",
    "        self.unique_notes = sorted(set(notes))\n",
    "        self.pitchdict = {note:number for number, note in enumerate(self.unique_notes)}\n",
    "        self.inverse_pitchdict = {number:note for number, note in enumerate(self.unique_notes)}\n",
    "        return list(map(self.pitchdict.get, notes))\n",
    "    \n",
    "    def get_training_sequences(self, sequence_length=100):\n",
    "        ins = []\n",
    "        out = []\n",
    "        notes = self.get_encoded_notes()\n",
    "        for i in range(0, len(notes) - sequence_length, 1):\n",
    "                ins.append(notes[i:i+sequence_length])\n",
    "                out.append(notes[i+sequence_length])\n",
    "        ins = np.expand_dims(np.array(ins), axis=2) / float(len(self.unique_notes))\n",
    "        out = to_categorical(out)\n",
    "        return ins, out\n",
    "    \n",
    "    def compile_model(self, inputs, latent_dim=256):\n",
    "        shape = (inputs.shape[1], inputs.shape[2])\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(latent_dim, input_shape=shape, recurrent_dropout=0.3, return_sequences=True))\n",
    "        model.add(LSTM(2 * latent_dim, recurrent_dropout=0.3, return_sequences=True))\n",
    "        model.add(LSTM(latent_dim))\n",
    "        model.add(BatchNorm())\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(latent_dim))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNorm())\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(len(self.unique_notes)))\n",
    "        model.add(Activation('softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "        self.model = model\n",
    "        return model\n",
    "    \n",
    "    def train(self, X, y, checkpoint='best_model.h5', epochs=100, batch_size=128, split=0.01, verbose=0):\n",
    "        mc = ModelCheckpoint(checkpoint, monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "        self.history = self.model.fit(X, y, epochs=epochs, batch_size=batch_size, \n",
    "                                      validation_split=split, verbose=verbose, \n",
    "                                      callbacks=[mc])\n",
    "        return self.history\n",
    "    \n",
    "    def predict(self, sequence, model_name='best_model.h5'):\n",
    "        sequence = list(np.squeeze(sequence))      \n",
    "        model = load_model(model_name)\n",
    "        output = []\n",
    "        for note_index in range(500):\n",
    "            inputs = np.reshape(sequence, (1, len(sequence), 1)) \n",
    "            inputs = inputs / float(len(self.unique_notes))\n",
    "            prediction = model.predict(inputs, verbose=0)\n",
    "            index = np.argmax(prediction)\n",
    "            output.append(self.inverse_pitchdict[index])\n",
    "            sequence.append(index)\n",
    "            sequence = sequence[1:len(sequence)]\n",
    "        return output\n",
    "    \n",
    "    def generate_notes(self, sequence, model_name='best_model.h5', outfile='test_output.mid'):\n",
    "        offset = 0\n",
    "        output_notes = []\n",
    "        sequence = list(np.squeeze(sequence))  \n",
    "        out_notes = self.predict(sequence, model_name)\n",
    "        for pattern in out_notes:\n",
    "            if ('.' in pattern) or pattern.isdigit():\n",
    "                notes_in_chord = pattern.split('.')\n",
    "                notes = []\n",
    "                for current_note in notes_in_chord:\n",
    "                    new_note = note.Note(int(current_note))\n",
    "                    new_note.storedInstrument = instrument.Piano()\n",
    "                    notes.append(new_note)\n",
    "                new = chord.Chord(notes)\n",
    "            else:\n",
    "                new = note.Note(pattern)\n",
    "                new.storedInstrument = instrument.Piano()        \n",
    "            new.offset = offset\n",
    "            output_notes.append(new)\n",
    "            offset += 0.5\n",
    "        midi_stream = stream.Stream(output_notes)\n",
    "        midi_stream.write('midi', fp=outfile)\n",
    "        print('New music has been generated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100, 256)          264192    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100, 512)          1574912   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               787456    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 358)               92006     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 358)               0         \n",
      "=================================================================\n",
      "Total params: 2,786,406\n",
      "Trainable params: 2,785,382\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "None\n",
      "time: 46.5 s\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 256\n",
    "latent_dim = 256\n",
    "sequence_length = 100\n",
    "checkpoint = 'best_model.h5'\n",
    "files = glob.glob('./songs/*.mid')\n",
    "music = Music(files)\n",
    "notes = music.get_encoded_notes()\n",
    "ins, out = music.get_training_sequences(sequence_length)\n",
    "model = music.compile_model(ins, latent_dim)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 56506 samples, validate on 571 samples\n",
      "Epoch 1/50\n",
      " 4864/56506 [=>............................] - ETA: 32:42 - loss: 6.2178"
     ]
    }
   ],
   "source": [
    "hist = music.train(ins, out, checkpoint, epochs, batch_size, verbose=1, split=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music.generate_notes(ins[50], outfile='new_music.mid')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
