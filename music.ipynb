{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autotime loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from init import *\n",
    "from music21 import converter, instrument, note, chord\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation\n",
    "from keras.layers import BatchNormalization as BatchNorm\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.13 ms\n"
     ]
    }
   ],
   "source": [
    "class Music():\n",
    "    def __init__(self, filenames):\n",
    "        self.files = filenames\n",
    "        \n",
    "    def get_musical_notes(self):\n",
    "        notes = []\n",
    "        for filename in self.files:\n",
    "            song = converter.parse(filename)\n",
    "            parts = instrument.partitionByInstrument(song)\n",
    "            notes_to_parse = None\n",
    "            if parts:\n",
    "                notes_to_parse = parts.parts[0].recurse()\n",
    "            else:\n",
    "                notes_to_parse = song.flat.notes\n",
    "                \n",
    "            for el in notes_to_parse:\n",
    "                if isinstance(el, note.Note):\n",
    "                    notes.append(str(el.pitch))\n",
    "                elif isinstance(el, chord.Chord):\n",
    "                    chords = '.'.join(str(n) for n in el.normalOrder)\n",
    "                    notes.append(chords)\n",
    "        return notes\n",
    "    \n",
    "    def get_encoded_notes(self):\n",
    "        notes = self.get_musical_notes()\n",
    "        self.unique_notes = sorted(set(notes))\n",
    "        self.pitchdict = {note:number for number, note in enumerate(self.unique_notes)}\n",
    "        return list(map(self.pitchdict.get, notes))\n",
    "    \n",
    "    def get_training_sequences(self, sequence_length=100):\n",
    "        ins = []\n",
    "        out = []\n",
    "        notes = self.get_encoded_notes()\n",
    "        for i in range(0, len(notes) - sequence_length, 1):\n",
    "                ins.append(notes[i:i+sequence_length])\n",
    "                out.append(notes[i+sequence_length])\n",
    "        ins = np.expand_dims(np.array(ins), axis=2) / float(len(self.unique_notes))\n",
    "        out = to_categorical(out)\n",
    "        return ins, out\n",
    "    \n",
    "    def compile_model(self, inputs, latent_dim=256):\n",
    "        shape = (inputs.shape[1], inputs.shape[2])\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(latent_dim, input_shape=shape, recurrent_dropout=0.3, return_sequences=True))\n",
    "        model.add(LSTM(2 * latent_dim, recurrent_dropout=0.3, return_sequences=True))\n",
    "        model.add(LSTM(latent_dim))\n",
    "        model.add(BatchNorm())\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(latent_dim))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNorm())\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(len(self.unique_notes)))\n",
    "        model.add(Activation('softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "        self.model = model\n",
    "        return model\n",
    "    \n",
    "    def train(self, X, y, checkpoint='best_model.h5', epochs=100, batch_size=128, split=0.01, verbose=0):\n",
    "        mc = ModelCheckpoint(checkpoint, monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "        self.history = self.model.fit(X, y, epochs=epochs, batch_size=batch_size, \n",
    "                                      validation_split=split, verbose=verbose, \n",
    "                                      callbacks=[mc])\n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100, 256)          264192    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100, 512)          1574912   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               787456    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 177)               45489     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 177)               0         \n",
      "=================================================================\n",
      "Total params: 2,739,889\n",
      "Trainable params: 2,738,865\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /usr/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 6615 samples, validate on 136 samples\n",
      "Epoch 1/50\n",
      "6615/6615 [==============================] - 215s 33ms/step - loss: 5.4159 - val_loss: 4.8916\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.89158, saving model to best_model.h5\n",
      "Epoch 2/50\n",
      "6615/6615 [==============================] - 243s 37ms/step - loss: 4.9942 - val_loss: 4.8529\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.89158 to 4.85288, saving model to best_model.h5\n",
      "Epoch 3/50\n",
      "6615/6615 [==============================] - 239s 36ms/step - loss: 4.8664 - val_loss: 4.7317\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.85288 to 4.73170, saving model to best_model.h5\n",
      "Epoch 4/50\n",
      "6615/6615 [==============================] - 248s 37ms/step - loss: 4.7469 - val_loss: 4.6215\n",
      "\n",
      "Epoch 00004: val_loss improved from 4.73170 to 4.62146, saving model to best_model.h5\n",
      "Epoch 5/50\n",
      "3072/6615 [============>.................] - ETA: 2:00 - loss: 4.6333"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 256\n",
    "latent_dim = 256\n",
    "sequence_length = 100\n",
    "checkpoint = 'best_model.h5'\n",
    "files = glob.glob('./songs/*.mid')[:10]\n",
    "music = Music(files)\n",
    "notes = music.get_encoded_notes()\n",
    "ins, out = music.get_training_sequences(sequence_length)\n",
    "model = music.compile_model(ins, latent_dim)\n",
    "print(model.summary())\n",
    "hist = music.train(ins, out, checkpoint, epochs, batch_size, verbose=1, split=0.02)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
