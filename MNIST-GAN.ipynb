{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autotime loaded.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from gan import GAN\n",
    "from init import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.35 ms\n"
     ]
    }
   ],
   "source": [
    "class GenerativeModel(object):\n",
    "    def __init__(self, x_train): \n",
    "        self.x_train = x_train        \n",
    "        _, self.img_rows, self.img_cols, self.channel = x_train.shape\n",
    "        self.DCGAN = GAN(self.img_rows, self.img_cols)\n",
    "        self.discriminator =  self.DCGAN.discriminator_model()\n",
    "        self.adversarial = self.DCGAN.adversarial_model()\n",
    "        self.generator = self.DCGAN.generator()\n",
    "\n",
    "    def train(self, train_steps=2000, batch_size=256, save_interval=0):\n",
    "        noise_input = None\n",
    "        if save_interval>0:\n",
    "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "        for i in range(train_steps):\n",
    "            images_train = self.x_train[np.random.randint(0,\n",
    "                self.x_train.shape[0], size=batch_size), :, :, :]\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            images_fake = self.generator.predict(noise)\n",
    "            x = np.concatenate((images_train, images_fake))\n",
    "            y = np.ones([2*batch_size, 1])\n",
    "            y[batch_size:, :] = 0\n",
    "            d_loss = self.discriminator.train_on_batch(x, y)\n",
    "\n",
    "            y = np.ones([batch_size, 1])\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            a_loss = self.adversarial.train_on_batch(noise, y)\n",
    "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
    "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
    "            print(log_mesg)\n",
    "            if save_interval>0:\n",
    "                if (i+1)%save_interval==0:\n",
    "                    self.plot_images(save2file=True, samples=noise_input.shape[0],\\\n",
    "                                     noise=noise_input, step=(i+1))\n",
    "\n",
    "    def plot_images(self, save2file=False, fake=True, samples=16, noise=None, step=0):\n",
    "        filename = './figs/mnist.png'\n",
    "        if fake:\n",
    "            if noise is None:\n",
    "                noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
    "            else:\n",
    "                filename = \"./figs/mnist_%d.png\" % step\n",
    "            images = self.generator.predict(noise)\n",
    "        else:\n",
    "            i = np.random.randint(0, self.x_train.shape[0], samples)\n",
    "            images = self.x_train[i, :, :, :]\n",
    "\n",
    "        plt.figure(figsize=(10,10))\n",
    "        for i in range(images.shape[0]):\n",
    "            plt.subplot(4, 4, i+1)\n",
    "            image = images[i, :, :, :]\n",
    "            image = np.reshape(image, [self.img_rows, self.img_cols])\n",
    "            plt.imshow(image, cmap='gray')\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        if save2file:\n",
    "            plt.savefig(filename)\n",
    "            plt.close('all')\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"Discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 2, 2, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 2049      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,305,409\n",
      "Trainable params: 4,305,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"Generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12544)             1266944   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 12544)             50176     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 14, 14, 128)       819328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 1)         801       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 2,394,241\n",
      "Trainable params: 2,368,705\n",
      "Non-trainable params: 25,536\n",
      "_________________________________________________________________\n",
      "time: 1.74 s\n"
     ]
    }
   ],
   "source": [
    "data = keras.datasets.mnist.load_data()[0][0]\n",
    "data = np.expand_dims(data, -1).astype(\"float32\") / 255\n",
    "model = GenerativeModel(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [D loss: 0.692765, acc: 0.505859]  [A loss: 0.898017, acc: 0.000000]\n",
      "1: [D loss: 0.656942, acc: 0.500000]  [A loss: 0.841465, acc: 0.000000]\n",
      "2: [D loss: 0.559883, acc: 1.000000]  [A loss: 0.722053, acc: 0.312500]\n",
      "3: [D loss: 0.496521, acc: 0.500000]  [A loss: 1.104102, acc: 0.000000]\n",
      "4: [D loss: 0.400987, acc: 0.994141]  [A loss: 0.454255, acc: 1.000000]\n",
      "5: [D loss: 0.230181, acc: 1.000000]  [A loss: 0.155492, acc: 1.000000]\n",
      "6: [D loss: 0.156674, acc: 1.000000]  [A loss: 0.074632, acc: 1.000000]\n",
      "7: [D loss: 0.105886, acc: 1.000000]  [A loss: 0.031525, acc: 1.000000]\n",
      "8: [D loss: 0.085447, acc: 1.000000]  [A loss: 0.012655, acc: 1.000000]\n",
      "9: [D loss: 0.070825, acc: 1.000000]  [A loss: 0.009446, acc: 1.000000]\n",
      "10: [D loss: 0.063245, acc: 1.000000]  [A loss: 0.005963, acc: 1.000000]\n",
      "11: [D loss: 0.056527, acc: 0.998047]  [A loss: 0.002634, acc: 1.000000]\n",
      "12: [D loss: 0.048844, acc: 1.000000]  [A loss: 0.002383, acc: 1.000000]\n",
      "13: [D loss: 0.041596, acc: 1.000000]  [A loss: 0.002847, acc: 1.000000]\n",
      "14: [D loss: 0.036884, acc: 0.998047]  [A loss: 0.000705, acc: 1.000000]\n",
      "15: [D loss: 0.029988, acc: 1.000000]  [A loss: 0.000923, acc: 1.000000]\n",
      "16: [D loss: 0.026281, acc: 0.998047]  [A loss: 0.000388, acc: 1.000000]\n",
      "17: [D loss: 0.027417, acc: 0.996094]  [A loss: 0.000046, acc: 1.000000]\n",
      "18: [D loss: 0.019902, acc: 1.000000]  [A loss: 0.001181, acc: 1.000000]\n",
      "19: [D loss: 0.014079, acc: 1.000000]  [A loss: 0.000535, acc: 1.000000]\n",
      "20: [D loss: 0.011947, acc: 1.000000]  [A loss: 0.001313, acc: 1.000000]\n",
      "21: [D loss: 0.013005, acc: 1.000000]  [A loss: 0.000014, acc: 1.000000]\n",
      "22: [D loss: 0.011280, acc: 1.000000]  [A loss: 0.000017, acc: 1.000000]\n",
      "23: [D loss: 0.007756, acc: 1.000000]  [A loss: 0.000047, acc: 1.000000]\n",
      "24: [D loss: 0.009285, acc: 0.998047]  [A loss: 0.000000, acc: 1.000000]\n",
      "25: [D loss: 0.008034, acc: 1.000000]  [A loss: 0.000014, acc: 1.000000]\n",
      "26: [D loss: 0.004744, acc: 1.000000]  [A loss: 0.000021, acc: 1.000000]\n",
      "27: [D loss: 0.006911, acc: 0.998047]  [A loss: 0.000000, acc: 1.000000]\n",
      "28: [D loss: 0.004950, acc: 1.000000]  [A loss: 0.000002, acc: 1.000000]\n",
      "29: [D loss: 0.003093, acc: 1.000000]  [A loss: 0.000003, acc: 1.000000]\n",
      "30: [D loss: 0.003888, acc: 0.998047]  [A loss: 0.000001, acc: 1.000000]\n",
      "31: [D loss: 0.002882, acc: 1.000000]  [A loss: 0.000005, acc: 1.000000]\n",
      "32: [D loss: 0.002078, acc: 1.000000]  [A loss: 0.000005, acc: 1.000000]\n",
      "33: [D loss: 0.002039, acc: 1.000000]  [A loss: 0.000003, acc: 1.000000]\n",
      "34: [D loss: 0.003497, acc: 0.998047]  [A loss: 0.000000, acc: 1.000000]\n",
      "35: [D loss: 0.002697, acc: 1.000000]  [A loss: 0.000000, acc: 1.000000]\n",
      "36: [D loss: 0.001723, acc: 1.000000]  [A loss: 0.000002, acc: 1.000000]\n",
      "37: [D loss: 0.001282, acc: 1.000000]  [A loss: 0.000008, acc: 1.000000]\n",
      "38: [D loss: 0.001100, acc: 1.000000]  [A loss: 0.000003, acc: 1.000000]\n",
      "39: [D loss: 0.003940, acc: 0.998047]  [A loss: 0.000000, acc: 1.000000]\n",
      "40: [D loss: 0.007627, acc: 1.000000]  [A loss: 0.000016, acc: 1.000000]\n",
      "41: [D loss: 0.000653, acc: 1.000000]  [A loss: 0.000068, acc: 1.000000]\n",
      "42: [D loss: 0.001371, acc: 1.000000]  [A loss: 0.000001, acc: 1.000000]\n",
      "43: [D loss: 0.001312, acc: 1.000000]  [A loss: 0.000468, acc: 1.000000]\n",
      "44: [D loss: 0.002169, acc: 1.000000]  [A loss: 0.000003, acc: 1.000000]\n",
      "45: [D loss: 0.006162, acc: 0.998047]  [A loss: 6.761283, acc: 0.031250]\n",
      "46: [D loss: 7.517165, acc: 0.503906]  [A loss: 27.985662, acc: 0.000000]\n",
      "47: [D loss: 2.296100, acc: 0.500000]  [A loss: 0.000001, acc: 1.000000]\n",
      "48: [D loss: 2.198546, acc: 0.544922]  [A loss: 0.020039, acc: 1.000000]\n",
      "49: [D loss: 0.455603, acc: 0.808594]  [A loss: 1.771260, acc: 0.195312]\n",
      "50: [D loss: 1.339425, acc: 0.582031]  [A loss: 8.797974, acc: 0.000000]\n",
      "51: [D loss: 0.430186, acc: 0.800781]  [A loss: 1.058503, acc: 0.421875]\n",
      "52: [D loss: 1.288557, acc: 0.560547]  [A loss: 11.417684, acc: 0.000000]\n",
      "53: [D loss: 0.710151, acc: 0.666016]  [A loss: 0.010583, acc: 1.000000]\n",
      "54: [D loss: 1.362984, acc: 0.546875]  [A loss: 6.694517, acc: 0.000000]\n",
      "55: [D loss: 0.329810, acc: 0.878906]  [A loss: 2.202533, acc: 0.070312]\n",
      "56: [D loss: 1.146321, acc: 0.558594]  [A loss: 9.319712, acc: 0.000000]\n",
      "57: [D loss: 0.540685, acc: 0.708984]  [A loss: 0.092853, acc: 0.996094]\n",
      "58: [D loss: 0.866453, acc: 0.585938]  [A loss: 6.110747, acc: 0.000000]\n",
      "59: [D loss: 0.286735, acc: 0.919922]  [A loss: 1.884060, acc: 0.058594]\n",
      "60: [D loss: 0.971602, acc: 0.568359]  [A loss: 7.830930, acc: 0.000000]\n",
      "61: [D loss: 0.372509, acc: 0.828125]  [A loss: 0.460268, acc: 0.812500]\n",
      "62: [D loss: 0.703300, acc: 0.628906]  [A loss: 5.279808, acc: 0.000000]\n",
      "63: [D loss: 0.235375, acc: 0.933594]  [A loss: 1.657524, acc: 0.121094]\n",
      "64: [D loss: 0.621723, acc: 0.636719]  [A loss: 4.585110, acc: 0.000000]\n",
      "65: [D loss: 0.221936, acc: 0.951172]  [A loss: 1.346503, acc: 0.222656]\n",
      "66: [D loss: 0.420111, acc: 0.781250]  [A loss: 1.950521, acc: 0.070312]\n",
      "67: [D loss: 0.351966, acc: 0.835938]  [A loss: 1.646122, acc: 0.195312]\n",
      "68: [D loss: 0.305674, acc: 0.865234]  [A loss: 1.102365, acc: 0.359375]\n",
      "69: [D loss: 0.376918, acc: 0.833984]  [A loss: 2.015378, acc: 0.089844]\n",
      "70: [D loss: 0.300661, acc: 0.873047]  [A loss: 1.016428, acc: 0.429688]\n",
      "71: [D loss: 0.342786, acc: 0.859375]  [A loss: 1.791924, acc: 0.156250]\n",
      "72: [D loss: 0.428407, acc: 0.777344]  [A loss: 3.416076, acc: 0.023438]\n",
      "73: [D loss: 0.312196, acc: 0.886719]  [A loss: 1.819519, acc: 0.183594]\n",
      "74: [D loss: 0.577914, acc: 0.730469]  [A loss: 5.927351, acc: 0.000000]\n",
      "75: [D loss: 0.266273, acc: 0.908203]  [A loss: 0.311329, acc: 0.875000]\n",
      "76: [D loss: 0.865113, acc: 0.621094]  [A loss: 8.044413, acc: 0.000000]\n",
      "77: [D loss: 0.601171, acc: 0.673828]  [A loss: 0.010858, acc: 1.000000]\n",
      "78: [D loss: 0.758824, acc: 0.673828]  [A loss: 0.890905, acc: 0.476562]\n",
      "79: [D loss: 0.939895, acc: 0.576172]  [A loss: 7.090107, acc: 0.000000]\n",
      "80: [D loss: 0.731914, acc: 0.601562]  [A loss: 0.137715, acc: 0.976562]\n",
      "81: [D loss: 1.108969, acc: 0.580078]  [A loss: 2.562431, acc: 0.027344]\n",
      "82: [D loss: 0.610777, acc: 0.712891]  [A loss: 3.628831, acc: 0.000000]\n",
      "83: [D loss: 0.507381, acc: 0.761719]  [A loss: 2.151266, acc: 0.039062]\n",
      "84: [D loss: 0.735348, acc: 0.630859]  [A loss: 5.066762, acc: 0.000000]\n",
      "85: [D loss: 0.631933, acc: 0.644531]  [A loss: 0.279782, acc: 0.933594]\n",
      "86: [D loss: 1.197274, acc: 0.507812]  [A loss: 4.155602, acc: 0.000000]\n",
      "87: [D loss: 0.579038, acc: 0.654297]  [A loss: 0.653014, acc: 0.632812]\n",
      "88: [D loss: 1.116473, acc: 0.531250]  [A loss: 4.086287, acc: 0.000000]\n",
      "89: [D loss: 0.629457, acc: 0.638672]  [A loss: 0.674227, acc: 0.601562]\n",
      "90: [D loss: 0.946887, acc: 0.521484]  [A loss: 3.201923, acc: 0.000000]\n",
      "91: [D loss: 0.534411, acc: 0.740234]  [A loss: 0.954392, acc: 0.324219]\n",
      "92: [D loss: 0.798737, acc: 0.542969]  [A loss: 3.249828, acc: 0.000000]\n",
      "93: [D loss: 0.519850, acc: 0.744141]  [A loss: 0.999086, acc: 0.292969]\n",
      "94: [D loss: 0.761328, acc: 0.533203]  [A loss: 3.031816, acc: 0.000000]\n",
      "95: [D loss: 0.519092, acc: 0.777344]  [A loss: 1.118959, acc: 0.140625]\n",
      "96: [D loss: 0.737720, acc: 0.550781]  [A loss: 2.990912, acc: 0.000000]\n",
      "97: [D loss: 0.503372, acc: 0.787109]  [A loss: 1.023860, acc: 0.210938]\n",
      "98: [D loss: 0.718755, acc: 0.558594]  [A loss: 3.013347, acc: 0.000000]\n",
      "99: [D loss: 0.465946, acc: 0.798828]  [A loss: 1.030886, acc: 0.187500]\n",
      "100: [D loss: 0.711798, acc: 0.542969]  [A loss: 2.871310, acc: 0.000000]\n",
      "101: [D loss: 0.506670, acc: 0.798828]  [A loss: 0.914066, acc: 0.324219]\n",
      "102: [D loss: 0.767162, acc: 0.517578]  [A loss: 2.749521, acc: 0.000000]\n",
      "103: [D loss: 0.496789, acc: 0.791016]  [A loss: 0.931867, acc: 0.304688]\n",
      "104: [D loss: 0.676623, acc: 0.550781]  [A loss: 2.608830, acc: 0.000000]\n",
      "105: [D loss: 0.478217, acc: 0.822266]  [A loss: 0.967037, acc: 0.257812]\n",
      "106: [D loss: 0.649913, acc: 0.566406]  [A loss: 2.518538, acc: 0.000000]\n",
      "107: [D loss: 0.482527, acc: 0.843750]  [A loss: 0.953584, acc: 0.226562]\n",
      "108: [D loss: 0.671620, acc: 0.558594]  [A loss: 2.541914, acc: 0.000000]\n",
      "109: [D loss: 0.496487, acc: 0.832031]  [A loss: 0.815199, acc: 0.429688]\n",
      "110: [D loss: 0.742618, acc: 0.531250]  [A loss: 2.475753, acc: 0.000000]\n",
      "111: [D loss: 0.512801, acc: 0.810547]  [A loss: 0.866562, acc: 0.320312]\n",
      "112: [D loss: 0.684102, acc: 0.552734]  [A loss: 2.226802, acc: 0.000000]\n",
      "113: [D loss: 0.496073, acc: 0.822266]  [A loss: 0.930341, acc: 0.269531]\n",
      "114: [D loss: 0.637993, acc: 0.556641]  [A loss: 2.257567, acc: 0.000000]\n",
      "115: [D loss: 0.481608, acc: 0.845703]  [A loss: 0.891744, acc: 0.308594]\n",
      "116: [D loss: 0.669056, acc: 0.541016]  [A loss: 2.382412, acc: 0.000000]\n",
      "117: [D loss: 0.491177, acc: 0.820312]  [A loss: 0.826477, acc: 0.398438]\n",
      "118: [D loss: 0.718936, acc: 0.529297]  [A loss: 2.409691, acc: 0.000000]\n",
      "119: [D loss: 0.512568, acc: 0.804688]  [A loss: 0.749319, acc: 0.460938]\n",
      "120: [D loss: 0.727857, acc: 0.511719]  [A loss: 2.168435, acc: 0.000000]\n",
      "121: [D loss: 0.505200, acc: 0.839844]  [A loss: 0.779570, acc: 0.406250]\n",
      "122: [D loss: 0.688347, acc: 0.544922]  [A loss: 2.105279, acc: 0.000000]\n",
      "123: [D loss: 0.497673, acc: 0.835938]  [A loss: 0.753071, acc: 0.464844]\n",
      "124: [D loss: 0.695390, acc: 0.537109]  [A loss: 1.974420, acc: 0.000000]\n",
      "125: [D loss: 0.502585, acc: 0.839844]  [A loss: 0.790370, acc: 0.449219]\n",
      "126: [D loss: 0.690563, acc: 0.523438]  [A loss: 2.175935, acc: 0.000000]\n",
      "127: [D loss: 0.511745, acc: 0.814453]  [A loss: 0.650060, acc: 0.632812]\n",
      "128: [D loss: 0.760692, acc: 0.521484]  [A loss: 2.037945, acc: 0.000000]\n",
      "129: [D loss: 0.516919, acc: 0.824219]  [A loss: 0.630961, acc: 0.667969]\n",
      "130: [D loss: 0.729091, acc: 0.527344]  [A loss: 1.857563, acc: 0.000000]\n",
      "131: [D loss: 0.520178, acc: 0.806641]  [A loss: 0.800393, acc: 0.382812]\n",
      "132: [D loss: 0.682586, acc: 0.527344]  [A loss: 1.855070, acc: 0.000000]\n",
      "133: [D loss: 0.532350, acc: 0.785156]  [A loss: 0.782284, acc: 0.394531]\n",
      "134: [D loss: 0.709391, acc: 0.517578]  [A loss: 2.009421, acc: 0.000000]\n",
      "135: [D loss: 0.540077, acc: 0.777344]  [A loss: 0.674389, acc: 0.574219]\n",
      "136: [D loss: 0.705897, acc: 0.511719]  [A loss: 1.910107, acc: 0.000000]\n",
      "137: [D loss: 0.552225, acc: 0.759766]  [A loss: 0.728292, acc: 0.453125]\n",
      "138: [D loss: 0.751573, acc: 0.511719]  [A loss: 1.940007, acc: 0.000000]\n",
      "139: [D loss: 0.571958, acc: 0.746094]  [A loss: 0.716150, acc: 0.500000]\n",
      "140: [D loss: 0.775398, acc: 0.500000]  [A loss: 2.092474, acc: 0.000000]\n",
      "141: [D loss: 0.574853, acc: 0.750000]  [A loss: 0.628406, acc: 0.660156]\n",
      "142: [D loss: 0.759913, acc: 0.500000]  [A loss: 1.714131, acc: 0.000000]\n",
      "143: [D loss: 0.586360, acc: 0.734375]  [A loss: 0.702883, acc: 0.542969]\n",
      "144: [D loss: 0.753616, acc: 0.500000]  [A loss: 1.712920, acc: 0.000000]\n",
      "145: [D loss: 0.581238, acc: 0.738281]  [A loss: 0.749007, acc: 0.394531]\n",
      "146: [D loss: 0.729355, acc: 0.496094]  [A loss: 1.639729, acc: 0.000000]\n",
      "147: [D loss: 0.577927, acc: 0.767578]  [A loss: 0.822587, acc: 0.308594]\n",
      "148: [D loss: 0.714112, acc: 0.507812]  [A loss: 1.845323, acc: 0.000000]\n",
      "149: [D loss: 0.583419, acc: 0.748047]  [A loss: 0.705774, acc: 0.507812]\n",
      "150: [D loss: 0.752567, acc: 0.496094]  [A loss: 1.801125, acc: 0.000000]\n",
      "151: [D loss: 0.582284, acc: 0.757812]  [A loss: 0.670027, acc: 0.597656]\n",
      "152: [D loss: 0.739538, acc: 0.505859]  [A loss: 1.689416, acc: 0.000000]\n",
      "153: [D loss: 0.575099, acc: 0.771484]  [A loss: 0.790040, acc: 0.339844]\n",
      "154: [D loss: 0.716290, acc: 0.501953]  [A loss: 1.712392, acc: 0.000000]\n",
      "155: [D loss: 0.588708, acc: 0.738281]  [A loss: 0.731889, acc: 0.445312]\n",
      "156: [D loss: 0.716730, acc: 0.503906]  [A loss: 1.653476, acc: 0.000000]\n",
      "157: [D loss: 0.578647, acc: 0.773438]  [A loss: 0.663020, acc: 0.601562]\n",
      "158: [D loss: 0.744152, acc: 0.500000]  [A loss: 1.696953, acc: 0.000000]\n",
      "159: [D loss: 0.582850, acc: 0.740234]  [A loss: 0.626025, acc: 0.687500]\n",
      "160: [D loss: 0.729436, acc: 0.500000]  [A loss: 1.555685, acc: 0.000000]\n",
      "161: [D loss: 0.587222, acc: 0.751953]  [A loss: 0.735676, acc: 0.441406]\n",
      "162: [D loss: 0.684524, acc: 0.509766]  [A loss: 1.591351, acc: 0.000000]\n",
      "163: [D loss: 0.575271, acc: 0.804688]  [A loss: 0.751140, acc: 0.417969]\n",
      "164: [D loss: 0.690703, acc: 0.507812]  [A loss: 1.607628, acc: 0.000000]\n",
      "165: [D loss: 0.578840, acc: 0.791016]  [A loss: 0.688659, acc: 0.531250]\n",
      "166: [D loss: 0.713828, acc: 0.509766]  [A loss: 1.660959, acc: 0.000000]\n",
      "167: [D loss: 0.588633, acc: 0.750000]  [A loss: 0.655326, acc: 0.593750]\n",
      "168: [D loss: 0.730675, acc: 0.501953]  [A loss: 1.594369, acc: 0.000000]\n",
      "169: [D loss: 0.583116, acc: 0.763672]  [A loss: 0.697004, acc: 0.496094]\n",
      "170: [D loss: 0.681408, acc: 0.509766]  [A loss: 1.551523, acc: 0.000000]\n",
      "171: [D loss: 0.584741, acc: 0.755859]  [A loss: 0.734430, acc: 0.460938]\n",
      "172: [D loss: 0.708073, acc: 0.513672]  [A loss: 1.835522, acc: 0.000000]\n",
      "173: [D loss: 0.582105, acc: 0.761719]  [A loss: 0.581470, acc: 0.753906]\n",
      "174: [D loss: 0.738181, acc: 0.498047]  [A loss: 1.604333, acc: 0.000000]\n",
      "175: [D loss: 0.589404, acc: 0.761719]  [A loss: 0.714278, acc: 0.457031]\n",
      "176: [D loss: 0.702904, acc: 0.509766]  [A loss: 1.597648, acc: 0.000000]\n",
      "177: [D loss: 0.580688, acc: 0.751953]  [A loss: 0.769324, acc: 0.339844]\n",
      "178: [D loss: 0.730394, acc: 0.498047]  [A loss: 1.904781, acc: 0.000000]\n",
      "179: [D loss: 0.586542, acc: 0.714844]  [A loss: 0.609996, acc: 0.718750]\n",
      "180: [D loss: 0.768979, acc: 0.500000]  [A loss: 1.554945, acc: 0.000000]\n",
      "181: [D loss: 0.614734, acc: 0.697266]  [A loss: 0.801921, acc: 0.292969]\n",
      "182: [D loss: 0.694761, acc: 0.507812]  [A loss: 1.513006, acc: 0.000000]\n",
      "183: [D loss: 0.607283, acc: 0.728516]  [A loss: 0.883885, acc: 0.156250]\n",
      "184: [D loss: 0.699350, acc: 0.521484]  [A loss: 1.761734, acc: 0.000000]\n",
      "185: [D loss: 0.589671, acc: 0.742188]  [A loss: 0.644409, acc: 0.648438]\n",
      "186: [D loss: 0.740006, acc: 0.513672]  [A loss: 1.574647, acc: 0.000000]\n",
      "187: [D loss: 0.626297, acc: 0.699219]  [A loss: 0.733454, acc: 0.394531]\n",
      "188: [D loss: 0.710176, acc: 0.513672]  [A loss: 1.506217, acc: 0.000000]\n",
      "189: [D loss: 0.607041, acc: 0.730469]  [A loss: 0.727727, acc: 0.441406]\n",
      "190: [D loss: 0.712615, acc: 0.509766]  [A loss: 1.574239, acc: 0.000000]\n",
      "191: [D loss: 0.627091, acc: 0.675781]  [A loss: 0.678668, acc: 0.515625]\n",
      "192: [D loss: 0.721046, acc: 0.503906]  [A loss: 1.346194, acc: 0.003906]\n",
      "193: [D loss: 0.623622, acc: 0.726562]  [A loss: 0.781382, acc: 0.292969]\n",
      "194: [D loss: 0.709654, acc: 0.507812]  [A loss: 1.365143, acc: 0.000000]\n",
      "195: [D loss: 0.611992, acc: 0.724609]  [A loss: 0.805347, acc: 0.289062]\n",
      "196: [D loss: 0.679072, acc: 0.542969]  [A loss: 1.356985, acc: 0.000000]\n",
      "197: [D loss: 0.608404, acc: 0.728516]  [A loss: 0.730550, acc: 0.460938]\n",
      "198: [D loss: 0.688655, acc: 0.529297]  [A loss: 1.505632, acc: 0.000000]\n",
      "199: [D loss: 0.623529, acc: 0.673828]  [A loss: 0.649960, acc: 0.636719]\n",
      "200: [D loss: 0.714846, acc: 0.517578]  [A loss: 1.374308, acc: 0.000000]\n",
      "201: [D loss: 0.623662, acc: 0.669922]  [A loss: 0.634557, acc: 0.687500]\n",
      "202: [D loss: 0.721475, acc: 0.501953]  [A loss: 1.346719, acc: 0.003906]\n",
      "203: [D loss: 0.620541, acc: 0.714844]  [A loss: 0.678549, acc: 0.546875]\n",
      "204: [D loss: 0.707954, acc: 0.519531]  [A loss: 1.454121, acc: 0.000000]\n",
      "205: [D loss: 0.610580, acc: 0.714844]  [A loss: 0.643524, acc: 0.675781]\n",
      "206: [D loss: 0.724477, acc: 0.521484]  [A loss: 1.455998, acc: 0.000000]\n",
      "207: [D loss: 0.619346, acc: 0.695312]  [A loss: 0.681577, acc: 0.542969]\n",
      "208: [D loss: 0.717410, acc: 0.519531]  [A loss: 1.383883, acc: 0.000000]\n",
      "209: [D loss: 0.614527, acc: 0.716797]  [A loss: 0.808363, acc: 0.312500]\n",
      "210: [D loss: 0.695880, acc: 0.554688]  [A loss: 1.559998, acc: 0.000000]\n",
      "211: [D loss: 0.612842, acc: 0.697266]  [A loss: 0.761801, acc: 0.402344]\n",
      "212: [D loss: 0.685015, acc: 0.556641]  [A loss: 1.483603, acc: 0.000000]\n",
      "213: [D loss: 0.599426, acc: 0.736328]  [A loss: 0.661454, acc: 0.597656]\n",
      "214: [D loss: 0.742828, acc: 0.525391]  [A loss: 1.630353, acc: 0.000000]\n",
      "215: [D loss: 0.613862, acc: 0.656250]  [A loss: 0.563070, acc: 0.769531]\n",
      "216: [D loss: 0.771423, acc: 0.511719]  [A loss: 1.204645, acc: 0.007812]\n",
      "217: [D loss: 0.642388, acc: 0.626953]  [A loss: 0.851825, acc: 0.210938]\n",
      "218: [D loss: 0.663231, acc: 0.562500]  [A loss: 1.253446, acc: 0.007812]\n",
      "219: [D loss: 0.624118, acc: 0.667969]  [A loss: 0.893167, acc: 0.183594]\n",
      "220: [D loss: 0.680625, acc: 0.544922]  [A loss: 1.604536, acc: 0.000000]\n",
      "221: [D loss: 0.587528, acc: 0.779297]  [A loss: 0.645245, acc: 0.632812]\n",
      "222: [D loss: 0.719490, acc: 0.517578]  [A loss: 1.571923, acc: 0.000000]\n",
      "223: [D loss: 0.630397, acc: 0.664062]  [A loss: 0.601833, acc: 0.718750]\n",
      "224: [D loss: 0.754534, acc: 0.509766]  [A loss: 1.371860, acc: 0.000000]\n",
      "225: [D loss: 0.619693, acc: 0.685547]  [A loss: 0.791484, acc: 0.351562]\n",
      "226: [D loss: 0.702711, acc: 0.525391]  [A loss: 1.597024, acc: 0.000000]\n",
      "227: [D loss: 0.607606, acc: 0.714844]  [A loss: 0.743222, acc: 0.417969]\n",
      "228: [D loss: 0.701573, acc: 0.515625]  [A loss: 1.464700, acc: 0.000000]\n",
      "229: [D loss: 0.628153, acc: 0.679688]  [A loss: 0.731659, acc: 0.468750]\n",
      "230: [D loss: 0.720642, acc: 0.529297]  [A loss: 1.659974, acc: 0.000000]\n",
      "231: [D loss: 0.633321, acc: 0.636719]  [A loss: 0.643682, acc: 0.621094]\n",
      "232: [D loss: 0.729526, acc: 0.511719]  [A loss: 1.321949, acc: 0.000000]\n",
      "233: [D loss: 0.626600, acc: 0.677734]  [A loss: 0.799576, acc: 0.304688]\n",
      "234: [D loss: 0.671356, acc: 0.552734]  [A loss: 1.239743, acc: 0.007812]\n",
      "235: [D loss: 0.627722, acc: 0.662109]  [A loss: 0.902893, acc: 0.167969]\n",
      "236: [D loss: 0.673232, acc: 0.541016]  [A loss: 1.361949, acc: 0.000000]\n",
      "237: [D loss: 0.623958, acc: 0.697266]  [A loss: 0.821815, acc: 0.285156]\n",
      "238: [D loss: 0.674181, acc: 0.556641]  [A loss: 1.615697, acc: 0.000000]\n",
      "239: [D loss: 0.615479, acc: 0.673828]  [A loss: 0.571577, acc: 0.738281]\n",
      "240: [D loss: 0.783807, acc: 0.507812]  [A loss: 1.609196, acc: 0.000000]\n",
      "241: [D loss: 0.645465, acc: 0.589844]  [A loss: 0.530839, acc: 0.859375]\n",
      "242: [D loss: 0.757433, acc: 0.494141]  [A loss: 1.065000, acc: 0.027344]\n",
      "243: [D loss: 0.657217, acc: 0.619141]  [A loss: 0.857800, acc: 0.199219]\n",
      "244: [D loss: 0.669423, acc: 0.564453]  [A loss: 1.133005, acc: 0.011719]\n",
      "245: [D loss: 0.633252, acc: 0.667969]  [A loss: 1.033811, acc: 0.042969]\n",
      "246: [D loss: 0.652969, acc: 0.597656]  [A loss: 1.255071, acc: 0.003906]\n",
      "247: [D loss: 0.629584, acc: 0.666016]  [A loss: 1.057779, acc: 0.050781]\n",
      "248: [D loss: 0.659305, acc: 0.562500]  [A loss: 1.545691, acc: 0.000000]\n",
      "249: [D loss: 0.615665, acc: 0.718750]  [A loss: 0.721231, acc: 0.460938]\n",
      "250: [D loss: 0.739262, acc: 0.507812]  [A loss: 1.899209, acc: 0.000000]\n",
      "251: [D loss: 0.667495, acc: 0.576172]  [A loss: 0.474069, acc: 0.941406]\n",
      "252: [D loss: 0.820529, acc: 0.501953]  [A loss: 1.112772, acc: 0.007812]\n",
      "253: [D loss: 0.655907, acc: 0.625000]  [A loss: 0.839164, acc: 0.140625]\n",
      "254: [D loss: 0.673344, acc: 0.564453]  [A loss: 1.052663, acc: 0.003906]\n",
      "255: [D loss: 0.633988, acc: 0.667969]  [A loss: 0.949321, acc: 0.097656]\n",
      "256: [D loss: 0.663603, acc: 0.566406]  [A loss: 1.115741, acc: 0.027344]\n",
      "257: [D loss: 0.643891, acc: 0.640625]  [A loss: 0.906927, acc: 0.125000]\n",
      "258: [D loss: 0.671951, acc: 0.548828]  [A loss: 1.241814, acc: 0.000000]\n",
      "259: [D loss: 0.628948, acc: 0.691406]  [A loss: 0.854249, acc: 0.191406]\n",
      "260: [D loss: 0.690828, acc: 0.531250]  [A loss: 1.617554, acc: 0.000000]\n",
      "261: [D loss: 0.624781, acc: 0.662109]  [A loss: 0.534358, acc: 0.859375]\n",
      "262: [D loss: 0.757796, acc: 0.501953]  [A loss: 1.367423, acc: 0.000000]\n",
      "263: [D loss: 0.650240, acc: 0.623047]  [A loss: 0.660664, acc: 0.582031]\n",
      "264: [D loss: 0.711445, acc: 0.498047]  [A loss: 1.138511, acc: 0.003906]\n",
      "265: [D loss: 0.642094, acc: 0.660156]  [A loss: 0.818855, acc: 0.234375]\n",
      "266: [D loss: 0.686371, acc: 0.515625]  [A loss: 1.225972, acc: 0.000000]\n",
      "267: [D loss: 0.634423, acc: 0.675781]  [A loss: 0.844134, acc: 0.171875]\n",
      "268: [D loss: 0.676821, acc: 0.546875]  [A loss: 1.200154, acc: 0.003906]\n",
      "269: [D loss: 0.636732, acc: 0.664062]  [A loss: 0.850227, acc: 0.199219]\n",
      "270: [D loss: 0.669536, acc: 0.552734]  [A loss: 1.207223, acc: 0.000000]\n",
      "271: [D loss: 0.641066, acc: 0.638672]  [A loss: 0.899315, acc: 0.113281]\n",
      "272: [D loss: 0.673028, acc: 0.544922]  [A loss: 1.396204, acc: 0.000000]\n",
      "273: [D loss: 0.632388, acc: 0.687500]  [A loss: 0.684670, acc: 0.554688]\n",
      "274: [D loss: 0.733527, acc: 0.503906]  [A loss: 1.644266, acc: 0.000000]\n",
      "275: [D loss: 0.660514, acc: 0.562500]  [A loss: 0.517319, acc: 0.917969]\n",
      "276: [D loss: 0.771544, acc: 0.498047]  [A loss: 1.068722, acc: 0.000000]\n",
      "277: [D loss: 0.647640, acc: 0.666016]  [A loss: 0.804443, acc: 0.203125]\n",
      "278: [D loss: 0.680340, acc: 0.537109]  [A loss: 1.026369, acc: 0.023438]\n",
      "279: [D loss: 0.658067, acc: 0.605469]  [A loss: 0.922217, acc: 0.093750]\n",
      "280: [D loss: 0.644192, acc: 0.630859]  [A loss: 1.007352, acc: 0.054688]\n",
      "281: [D loss: 0.653224, acc: 0.601562]  [A loss: 1.013606, acc: 0.019531]\n",
      "282: [D loss: 0.636216, acc: 0.652344]  [A loss: 0.986366, acc: 0.039062]\n",
      "283: [D loss: 0.663500, acc: 0.572266]  [A loss: 1.201299, acc: 0.000000]\n",
      "284: [D loss: 0.640431, acc: 0.669922]  [A loss: 0.939310, acc: 0.128906]\n",
      "285: [D loss: 0.687850, acc: 0.537109]  [A loss: 1.646282, acc: 0.000000]\n",
      "286: [D loss: 0.641320, acc: 0.599609]  [A loss: 0.479992, acc: 0.937500]\n",
      "287: [D loss: 0.839971, acc: 0.500000]  [A loss: 1.337223, acc: 0.000000]\n",
      "288: [D loss: 0.675455, acc: 0.562500]  [A loss: 0.678425, acc: 0.550781]\n",
      "289: [D loss: 0.716310, acc: 0.511719]  [A loss: 0.997257, acc: 0.027344]\n",
      "290: [D loss: 0.654898, acc: 0.644531]  [A loss: 0.899097, acc: 0.128906]\n",
      "291: [D loss: 0.661840, acc: 0.580078]  [A loss: 1.058576, acc: 0.031250]\n",
      "292: [D loss: 0.644917, acc: 0.666016]  [A loss: 0.924966, acc: 0.089844]\n",
      "293: [D loss: 0.676597, acc: 0.566406]  [A loss: 1.110846, acc: 0.019531]\n",
      "294: [D loss: 0.651419, acc: 0.652344]  [A loss: 0.851397, acc: 0.187500]\n",
      "295: [D loss: 0.664384, acc: 0.544922]  [A loss: 1.296916, acc: 0.000000]\n",
      "296: [D loss: 0.651923, acc: 0.630859]  [A loss: 0.699695, acc: 0.480469]\n",
      "297: [D loss: 0.701939, acc: 0.517578]  [A loss: 1.519712, acc: 0.000000]\n",
      "298: [D loss: 0.645161, acc: 0.611328]  [A loss: 0.556783, acc: 0.835938]\n",
      "299: [D loss: 0.769825, acc: 0.501953]  [A loss: 1.251811, acc: 0.000000]\n",
      "300: [D loss: 0.652861, acc: 0.607422]  [A loss: 0.731389, acc: 0.421875]\n",
      "301: [D loss: 0.682812, acc: 0.550781]  [A loss: 1.013256, acc: 0.019531]\n",
      "302: [D loss: 0.646029, acc: 0.648438]  [A loss: 0.857233, acc: 0.171875]\n",
      "303: [D loss: 0.658208, acc: 0.605469]  [A loss: 1.039066, acc: 0.031250]\n",
      "304: [D loss: 0.647702, acc: 0.619141]  [A loss: 0.897843, acc: 0.128906]\n",
      "305: [D loss: 0.674002, acc: 0.574219]  [A loss: 1.161036, acc: 0.007812]\n",
      "306: [D loss: 0.636954, acc: 0.666016]  [A loss: 0.846377, acc: 0.203125]\n",
      "307: [D loss: 0.645982, acc: 0.576172]  [A loss: 1.285220, acc: 0.000000]\n",
      "308: [D loss: 0.629714, acc: 0.683594]  [A loss: 0.714110, acc: 0.496094]\n",
      "309: [D loss: 0.716050, acc: 0.507812]  [A loss: 1.571490, acc: 0.000000]\n",
      "310: [D loss: 0.650922, acc: 0.589844]  [A loss: 0.511561, acc: 0.910156]\n",
      "311: [D loss: 0.763816, acc: 0.503906]  [A loss: 1.115956, acc: 0.019531]\n",
      "312: [D loss: 0.662714, acc: 0.630859]  [A loss: 0.724966, acc: 0.460938]\n",
      "313: [D loss: 0.688121, acc: 0.511719]  [A loss: 0.979746, acc: 0.078125]\n",
      "314: [D loss: 0.654669, acc: 0.628906]  [A loss: 0.871234, acc: 0.160156]\n",
      "315: [D loss: 0.662322, acc: 0.583984]  [A loss: 1.059618, acc: 0.031250]\n",
      "316: [D loss: 0.641265, acc: 0.654297]  [A loss: 0.809049, acc: 0.285156]\n",
      "317: [D loss: 0.662420, acc: 0.574219]  [A loss: 1.140079, acc: 0.003906]\n",
      "318: [D loss: 0.644546, acc: 0.652344]  [A loss: 0.810199, acc: 0.250000]\n",
      "319: [D loss: 0.671314, acc: 0.537109]  [A loss: 1.355808, acc: 0.000000]\n",
      "320: [D loss: 0.630025, acc: 0.707031]  [A loss: 0.742136, acc: 0.425781]\n",
      "321: [D loss: 0.718584, acc: 0.521484]  [A loss: 1.571482, acc: 0.000000]\n",
      "322: [D loss: 0.677750, acc: 0.541016]  [A loss: 0.537234, acc: 0.851562]\n",
      "323: [D loss: 0.745335, acc: 0.500000]  [A loss: 1.134528, acc: 0.000000]\n",
      "324: [D loss: 0.645818, acc: 0.646484]  [A loss: 0.765777, acc: 0.339844]\n",
      "325: [D loss: 0.686145, acc: 0.539062]  [A loss: 1.026397, acc: 0.035156]\n",
      "326: [D loss: 0.649720, acc: 0.625000]  [A loss: 0.850113, acc: 0.199219]\n",
      "327: [D loss: 0.665821, acc: 0.582031]  [A loss: 1.069887, acc: 0.011719]\n",
      "328: [D loss: 0.647078, acc: 0.634766]  [A loss: 0.861184, acc: 0.140625]\n",
      "329: [D loss: 0.678576, acc: 0.564453]  [A loss: 1.191051, acc: 0.007812]\n",
      "330: [D loss: 0.651165, acc: 0.642578]  [A loss: 0.789342, acc: 0.320312]\n",
      "331: [D loss: 0.681590, acc: 0.542969]  [A loss: 1.344101, acc: 0.000000]\n",
      "332: [D loss: 0.637354, acc: 0.642578]  [A loss: 0.661528, acc: 0.601562]\n",
      "333: [D loss: 0.730353, acc: 0.507812]  [A loss: 1.452824, acc: 0.000000]\n",
      "334: [D loss: 0.666224, acc: 0.605469]  [A loss: 0.595809, acc: 0.750000]\n",
      "335: [D loss: 0.745025, acc: 0.501953]  [A loss: 1.185338, acc: 0.003906]\n",
      "336: [D loss: 0.652052, acc: 0.613281]  [A loss: 0.737036, acc: 0.378906]\n",
      "337: [D loss: 0.684861, acc: 0.550781]  [A loss: 1.015094, acc: 0.015625]\n",
      "338: [D loss: 0.652370, acc: 0.636719]  [A loss: 0.834827, acc: 0.195312]\n",
      "339: [D loss: 0.672675, acc: 0.589844]  [A loss: 1.060113, acc: 0.007812]\n",
      "340: [D loss: 0.661946, acc: 0.611328]  [A loss: 0.855483, acc: 0.175781]\n",
      "341: [D loss: 0.658961, acc: 0.595703]  [A loss: 1.029172, acc: 0.046875]\n",
      "342: [D loss: 0.649381, acc: 0.613281]  [A loss: 0.933347, acc: 0.097656]\n",
      "343: [D loss: 0.667600, acc: 0.589844]  [A loss: 1.041758, acc: 0.042969]\n",
      "344: [D loss: 0.654529, acc: 0.638672]  [A loss: 0.932414, acc: 0.136719]\n",
      "345: [D loss: 0.666145, acc: 0.582031]  [A loss: 1.136247, acc: 0.003906]\n",
      "346: [D loss: 0.643499, acc: 0.658203]  [A loss: 0.829554, acc: 0.234375]\n",
      "347: [D loss: 0.694713, acc: 0.541016]  [A loss: 1.470970, acc: 0.000000]\n",
      "348: [D loss: 0.649691, acc: 0.619141]  [A loss: 0.548681, acc: 0.855469]\n",
      "349: [D loss: 0.745769, acc: 0.501953]  [A loss: 1.322324, acc: 0.000000]\n",
      "350: [D loss: 0.659656, acc: 0.578125]  [A loss: 0.628950, acc: 0.671875]\n",
      "351: [D loss: 0.704819, acc: 0.521484]  [A loss: 1.002849, acc: 0.042969]\n",
      "352: [D loss: 0.660417, acc: 0.630859]  [A loss: 0.784564, acc: 0.292969]\n",
      "353: [D loss: 0.666129, acc: 0.585938]  [A loss: 1.002420, acc: 0.031250]\n",
      "354: [D loss: 0.649825, acc: 0.664062]  [A loss: 0.823225, acc: 0.203125]\n",
      "355: [D loss: 0.674195, acc: 0.562500]  [A loss: 1.095464, acc: 0.011719]\n",
      "356: [D loss: 0.647229, acc: 0.638672]  [A loss: 0.836600, acc: 0.246094]\n",
      "357: [D loss: 0.671840, acc: 0.562500]  [A loss: 1.121652, acc: 0.019531]\n",
      "358: [D loss: 0.647080, acc: 0.626953]  [A loss: 0.819603, acc: 0.273438]\n",
      "359: [D loss: 0.671933, acc: 0.566406]  [A loss: 1.190602, acc: 0.000000]\n",
      "360: [D loss: 0.655767, acc: 0.630859]  [A loss: 0.732986, acc: 0.429688]\n",
      "361: [D loss: 0.689056, acc: 0.531250]  [A loss: 1.210009, acc: 0.000000]\n",
      "362: [D loss: 0.650051, acc: 0.625000]  [A loss: 0.649331, acc: 0.632812]\n",
      "363: [D loss: 0.711696, acc: 0.527344]  [A loss: 1.190956, acc: 0.011719]\n",
      "364: [D loss: 0.659034, acc: 0.611328]  [A loss: 0.693737, acc: 0.511719]\n",
      "365: [D loss: 0.698049, acc: 0.519531]  [A loss: 1.127302, acc: 0.015625]\n",
      "366: [D loss: 0.663870, acc: 0.603516]  [A loss: 0.716993, acc: 0.414062]\n",
      "367: [D loss: 0.689867, acc: 0.542969]  [A loss: 1.006822, acc: 0.039062]\n",
      "368: [D loss: 0.649320, acc: 0.636719]  [A loss: 0.790885, acc: 0.300781]\n",
      "369: [D loss: 0.692577, acc: 0.517578]  [A loss: 1.105171, acc: 0.011719]\n",
      "370: [D loss: 0.656490, acc: 0.650391]  [A loss: 0.786873, acc: 0.312500]\n",
      "371: [D loss: 0.671912, acc: 0.566406]  [A loss: 1.164390, acc: 0.015625]\n",
      "372: [D loss: 0.657233, acc: 0.603516]  [A loss: 0.738629, acc: 0.433594]\n",
      "373: [D loss: 0.697595, acc: 0.531250]  [A loss: 1.172637, acc: 0.015625]\n",
      "374: [D loss: 0.644392, acc: 0.644531]  [A loss: 0.676333, acc: 0.570312]\n",
      "375: [D loss: 0.701742, acc: 0.515625]  [A loss: 1.210359, acc: 0.015625]\n",
      "376: [D loss: 0.672497, acc: 0.593750]  [A loss: 0.715754, acc: 0.511719]\n",
      "377: [D loss: 0.706922, acc: 0.505859]  [A loss: 1.096544, acc: 0.007812]\n",
      "378: [D loss: 0.656041, acc: 0.609375]  [A loss: 0.763523, acc: 0.351562]\n",
      "379: [D loss: 0.664842, acc: 0.552734]  [A loss: 1.045705, acc: 0.035156]\n",
      "380: [D loss: 0.652198, acc: 0.623047]  [A loss: 0.842332, acc: 0.199219]\n",
      "381: [D loss: 0.687671, acc: 0.560547]  [A loss: 1.065827, acc: 0.027344]\n",
      "382: [D loss: 0.658831, acc: 0.605469]  [A loss: 0.826519, acc: 0.242188]\n",
      "383: [D loss: 0.689350, acc: 0.562500]  [A loss: 1.166878, acc: 0.019531]\n",
      "384: [D loss: 0.651653, acc: 0.642578]  [A loss: 0.704597, acc: 0.484375]\n",
      "385: [D loss: 0.700862, acc: 0.521484]  [A loss: 1.228529, acc: 0.011719]\n",
      "386: [D loss: 0.661973, acc: 0.601562]  [A loss: 0.677442, acc: 0.589844]\n",
      "387: [D loss: 0.728437, acc: 0.517578]  [A loss: 1.152619, acc: 0.007812]\n",
      "388: [D loss: 0.667783, acc: 0.578125]  [A loss: 0.696637, acc: 0.539062]\n",
      "389: [D loss: 0.707708, acc: 0.513672]  [A loss: 1.047679, acc: 0.019531]\n",
      "390: [D loss: 0.673330, acc: 0.562500]  [A loss: 0.730805, acc: 0.410156]\n",
      "391: [D loss: 0.675548, acc: 0.535156]  [A loss: 0.977575, acc: 0.039062]\n",
      "392: [D loss: 0.670666, acc: 0.572266]  [A loss: 0.799508, acc: 0.250000]\n",
      "393: [D loss: 0.657036, acc: 0.619141]  [A loss: 0.955637, acc: 0.066406]\n",
      "394: [D loss: 0.652878, acc: 0.623047]  [A loss: 0.867580, acc: 0.171875]\n",
      "395: [D loss: 0.666103, acc: 0.580078]  [A loss: 1.055357, acc: 0.039062]\n",
      "396: [D loss: 0.659655, acc: 0.619141]  [A loss: 0.802394, acc: 0.269531]\n",
      "397: [D loss: 0.676436, acc: 0.556641]  [A loss: 1.093042, acc: 0.023438]\n",
      "398: [D loss: 0.658657, acc: 0.605469]  [A loss: 0.757899, acc: 0.371094]\n",
      "399: [D loss: 0.690541, acc: 0.525391]  [A loss: 1.193711, acc: 0.011719]\n",
      "400: [D loss: 0.661169, acc: 0.605469]  [A loss: 0.703743, acc: 0.500000]\n",
      "401: [D loss: 0.696920, acc: 0.527344]  [A loss: 1.168610, acc: 0.003906]\n",
      "402: [D loss: 0.654054, acc: 0.636719]  [A loss: 0.709857, acc: 0.476562]\n",
      "403: [D loss: 0.685135, acc: 0.542969]  [A loss: 1.115893, acc: 0.015625]\n",
      "404: [D loss: 0.662932, acc: 0.619141]  [A loss: 0.723041, acc: 0.457031]\n",
      "405: [D loss: 0.696359, acc: 0.537109]  [A loss: 1.064624, acc: 0.015625]\n",
      "406: [D loss: 0.656331, acc: 0.638672]  [A loss: 0.691969, acc: 0.542969]\n",
      "407: [D loss: 0.702985, acc: 0.523438]  [A loss: 1.113512, acc: 0.003906]\n",
      "408: [D loss: 0.658758, acc: 0.634766]  [A loss: 0.712814, acc: 0.464844]\n",
      "409: [D loss: 0.697111, acc: 0.517578]  [A loss: 1.034865, acc: 0.019531]\n",
      "410: [D loss: 0.655453, acc: 0.660156]  [A loss: 0.747555, acc: 0.378906]\n",
      "411: [D loss: 0.688879, acc: 0.535156]  [A loss: 0.997024, acc: 0.023438]\n",
      "412: [D loss: 0.660220, acc: 0.599609]  [A loss: 0.790941, acc: 0.277344]\n",
      "413: [D loss: 0.670732, acc: 0.572266]  [A loss: 1.005321, acc: 0.054688]\n",
      "414: [D loss: 0.670100, acc: 0.601562]  [A loss: 0.888895, acc: 0.082031]\n",
      "415: [D loss: 0.682013, acc: 0.568359]  [A loss: 1.059926, acc: 0.011719]\n",
      "416: [D loss: 0.667389, acc: 0.585938]  [A loss: 0.838551, acc: 0.175781]\n",
      "417: [D loss: 0.685310, acc: 0.554688]  [A loss: 1.042191, acc: 0.019531]\n",
      "418: [D loss: 0.665868, acc: 0.578125]  [A loss: 0.770129, acc: 0.312500]\n",
      "419: [D loss: 0.688153, acc: 0.541016]  [A loss: 1.074624, acc: 0.015625]\n",
      "420: [D loss: 0.656478, acc: 0.628906]  [A loss: 0.768799, acc: 0.335938]\n",
      "421: [D loss: 0.702254, acc: 0.546875]  [A loss: 1.148060, acc: 0.015625]\n",
      "422: [D loss: 0.658258, acc: 0.615234]  [A loss: 0.687950, acc: 0.519531]\n",
      "423: [D loss: 0.710948, acc: 0.521484]  [A loss: 1.185905, acc: 0.003906]\n",
      "424: [D loss: 0.647411, acc: 0.617188]  [A loss: 0.668308, acc: 0.546875]\n",
      "425: [D loss: 0.705235, acc: 0.533203]  [A loss: 1.027081, acc: 0.015625]\n",
      "426: [D loss: 0.668260, acc: 0.589844]  [A loss: 0.766143, acc: 0.351562]\n",
      "427: [D loss: 0.670853, acc: 0.554688]  [A loss: 1.054790, acc: 0.019531]\n",
      "428: [D loss: 0.666236, acc: 0.615234]  [A loss: 0.841355, acc: 0.175781]\n",
      "429: [D loss: 0.673848, acc: 0.564453]  [A loss: 0.934259, acc: 0.070312]\n",
      "430: [D loss: 0.662849, acc: 0.607422]  [A loss: 0.891693, acc: 0.125000]\n",
      "431: [D loss: 0.666630, acc: 0.603516]  [A loss: 0.986711, acc: 0.050781]\n",
      "432: [D loss: 0.674813, acc: 0.599609]  [A loss: 0.900503, acc: 0.109375]\n",
      "433: [D loss: 0.675426, acc: 0.564453]  [A loss: 0.993517, acc: 0.050781]\n",
      "434: [D loss: 0.665782, acc: 0.593750]  [A loss: 0.896128, acc: 0.117188]\n",
      "435: [D loss: 0.670095, acc: 0.572266]  [A loss: 0.976824, acc: 0.082031]\n",
      "436: [D loss: 0.665151, acc: 0.599609]  [A loss: 0.923911, acc: 0.058594]\n",
      "437: [D loss: 0.678289, acc: 0.578125]  [A loss: 1.134173, acc: 0.015625]\n",
      "438: [D loss: 0.656969, acc: 0.638672]  [A loss: 0.723273, acc: 0.453125]\n",
      "439: [D loss: 0.694130, acc: 0.541016]  [A loss: 1.283095, acc: 0.000000]\n",
      "440: [D loss: 0.666087, acc: 0.585938]  [A loss: 0.592825, acc: 0.800781]\n",
      "441: [D loss: 0.732939, acc: 0.505859]  [A loss: 1.167935, acc: 0.000000]\n",
      "442: [D loss: 0.660584, acc: 0.599609]  [A loss: 0.685346, acc: 0.554688]\n",
      "443: [D loss: 0.695125, acc: 0.523438]  [A loss: 0.995404, acc: 0.039062]\n",
      "444: [D loss: 0.663673, acc: 0.630859]  [A loss: 0.771018, acc: 0.316406]\n",
      "445: [D loss: 0.675696, acc: 0.558594]  [A loss: 0.914507, acc: 0.105469]\n",
      "446: [D loss: 0.660299, acc: 0.613281]  [A loss: 0.872980, acc: 0.160156]\n",
      "447: [D loss: 0.669772, acc: 0.597656]  [A loss: 0.927547, acc: 0.105469]\n",
      "448: [D loss: 0.654848, acc: 0.617188]  [A loss: 0.857687, acc: 0.156250]\n",
      "449: [D loss: 0.677707, acc: 0.542969]  [A loss: 0.982244, acc: 0.054688]\n",
      "450: [D loss: 0.666112, acc: 0.582031]  [A loss: 0.824524, acc: 0.226562]\n",
      "451: [D loss: 0.655456, acc: 0.605469]  [A loss: 1.055175, acc: 0.031250]\n",
      "452: [D loss: 0.657293, acc: 0.621094]  [A loss: 0.760586, acc: 0.339844]\n",
      "453: [D loss: 0.683471, acc: 0.542969]  [A loss: 1.112256, acc: 0.027344]\n",
      "454: [D loss: 0.660097, acc: 0.634766]  [A loss: 0.709527, acc: 0.476562]\n",
      "455: [D loss: 0.692683, acc: 0.541016]  [A loss: 1.168896, acc: 0.007812]\n",
      "456: [D loss: 0.650897, acc: 0.636719]  [A loss: 0.688260, acc: 0.503906]\n",
      "457: [D loss: 0.727121, acc: 0.511719]  [A loss: 1.158491, acc: 0.011719]\n",
      "458: [D loss: 0.659525, acc: 0.587891]  [A loss: 0.725915, acc: 0.425781]\n",
      "459: [D loss: 0.697089, acc: 0.527344]  [A loss: 1.013660, acc: 0.035156]\n",
      "460: [D loss: 0.659827, acc: 0.644531]  [A loss: 0.741608, acc: 0.429688]\n",
      "461: [D loss: 0.677934, acc: 0.535156]  [A loss: 0.951750, acc: 0.117188]\n",
      "462: [D loss: 0.664017, acc: 0.583984]  [A loss: 0.851797, acc: 0.203125]\n",
      "463: [D loss: 0.672097, acc: 0.574219]  [A loss: 0.970608, acc: 0.109375]\n",
      "464: [D loss: 0.653959, acc: 0.628906]  [A loss: 0.911247, acc: 0.136719]\n",
      "465: [D loss: 0.670937, acc: 0.574219]  [A loss: 0.962920, acc: 0.046875]\n",
      "466: [D loss: 0.676839, acc: 0.560547]  [A loss: 0.931254, acc: 0.085938]\n",
      "467: [D loss: 0.670813, acc: 0.591797]  [A loss: 0.973199, acc: 0.062500]\n",
      "468: [D loss: 0.663541, acc: 0.617188]  [A loss: 0.896528, acc: 0.148438]\n",
      "469: [D loss: 0.668937, acc: 0.589844]  [A loss: 1.037134, acc: 0.031250]\n",
      "470: [D loss: 0.647524, acc: 0.636719]  [A loss: 0.878354, acc: 0.167969]\n",
      "471: [D loss: 0.662067, acc: 0.603516]  [A loss: 1.079248, acc: 0.035156]\n",
      "472: [D loss: 0.656873, acc: 0.626953]  [A loss: 0.715208, acc: 0.488281]\n",
      "473: [D loss: 0.692576, acc: 0.546875]  [A loss: 1.340966, acc: 0.003906]\n",
      "474: [D loss: 0.663483, acc: 0.582031]  [A loss: 0.558843, acc: 0.859375]\n",
      "475: [D loss: 0.751103, acc: 0.507812]  [A loss: 1.214302, acc: 0.015625]\n",
      "476: [D loss: 0.666538, acc: 0.587891]  [A loss: 0.675497, acc: 0.578125]\n",
      "477: [D loss: 0.696115, acc: 0.525391]  [A loss: 0.976433, acc: 0.058594]\n",
      "478: [D loss: 0.657488, acc: 0.625000]  [A loss: 0.777887, acc: 0.285156]\n",
      "479: [D loss: 0.672350, acc: 0.568359]  [A loss: 0.948058, acc: 0.054688]\n",
      "480: [D loss: 0.661085, acc: 0.611328]  [A loss: 0.807857, acc: 0.250000]\n",
      "481: [D loss: 0.668063, acc: 0.576172]  [A loss: 0.950054, acc: 0.074219]\n",
      "482: [D loss: 0.652558, acc: 0.626953]  [A loss: 0.861925, acc: 0.164062]\n",
      "483: [D loss: 0.682482, acc: 0.548828]  [A loss: 0.963780, acc: 0.105469]\n",
      "484: [D loss: 0.673420, acc: 0.576172]  [A loss: 0.877447, acc: 0.152344]\n",
      "485: [D loss: 0.668080, acc: 0.582031]  [A loss: 1.053310, acc: 0.039062]\n",
      "486: [D loss: 0.661305, acc: 0.634766]  [A loss: 0.832127, acc: 0.214844]\n",
      "487: [D loss: 0.673173, acc: 0.582031]  [A loss: 1.040037, acc: 0.070312]\n",
      "488: [D loss: 0.655831, acc: 0.611328]  [A loss: 0.716711, acc: 0.500000]\n",
      "489: [D loss: 0.705781, acc: 0.541016]  [A loss: 1.159117, acc: 0.003906]\n",
      "490: [D loss: 0.651773, acc: 0.630859]  [A loss: 0.685831, acc: 0.597656]\n",
      "491: [D loss: 0.689320, acc: 0.539062]  [A loss: 1.074667, acc: 0.035156]\n",
      "492: [D loss: 0.646999, acc: 0.642578]  [A loss: 0.726660, acc: 0.429688]\n",
      "493: [D loss: 0.707198, acc: 0.525391]  [A loss: 1.081410, acc: 0.015625]\n",
      "494: [D loss: 0.675119, acc: 0.570312]  [A loss: 0.743162, acc: 0.378906]\n",
      "495: [D loss: 0.687983, acc: 0.542969]  [A loss: 0.986953, acc: 0.050781]\n",
      "496: [D loss: 0.668986, acc: 0.578125]  [A loss: 0.780800, acc: 0.300781]\n",
      "497: [D loss: 0.691174, acc: 0.533203]  [A loss: 1.036657, acc: 0.035156]\n",
      "498: [D loss: 0.657169, acc: 0.632812]  [A loss: 0.775034, acc: 0.324219]\n",
      "499: [D loss: 0.688804, acc: 0.537109]  [A loss: 1.059715, acc: 0.031250]\n",
      "500: [D loss: 0.661750, acc: 0.613281]  [A loss: 0.770020, acc: 0.367188]\n",
      "501: [D loss: 0.680278, acc: 0.580078]  [A loss: 1.013901, acc: 0.046875]\n",
      "502: [D loss: 0.664050, acc: 0.613281]  [A loss: 0.773316, acc: 0.335938]\n",
      "503: [D loss: 0.675093, acc: 0.568359]  [A loss: 1.023587, acc: 0.046875]\n",
      "504: [D loss: 0.658647, acc: 0.601562]  [A loss: 0.782795, acc: 0.273438]\n",
      "505: [D loss: 0.682553, acc: 0.519531]  [A loss: 1.049833, acc: 0.023438]\n",
      "506: [D loss: 0.659766, acc: 0.630859]  [A loss: 0.772846, acc: 0.308594]\n",
      "507: [D loss: 0.678224, acc: 0.546875]  [A loss: 1.075955, acc: 0.023438]\n",
      "508: [D loss: 0.668021, acc: 0.609375]  [A loss: 0.743862, acc: 0.417969]\n",
      "509: [D loss: 0.683281, acc: 0.552734]  [A loss: 1.053368, acc: 0.054688]\n",
      "510: [D loss: 0.653409, acc: 0.615234]  [A loss: 0.766647, acc: 0.359375]\n",
      "511: [D loss: 0.680776, acc: 0.558594]  [A loss: 1.105576, acc: 0.031250]\n",
      "512: [D loss: 0.658825, acc: 0.605469]  [A loss: 0.850881, acc: 0.218750]\n",
      "513: [D loss: 0.660781, acc: 0.583984]  [A loss: 1.013229, acc: 0.093750]\n",
      "514: [D loss: 0.670379, acc: 0.576172]  [A loss: 0.806330, acc: 0.292969]\n",
      "515: [D loss: 0.680552, acc: 0.556641]  [A loss: 1.040923, acc: 0.058594]\n",
      "516: [D loss: 0.656713, acc: 0.609375]  [A loss: 0.796948, acc: 0.296875]\n",
      "517: [D loss: 0.672642, acc: 0.572266]  [A loss: 1.023287, acc: 0.062500]\n",
      "518: [D loss: 0.671428, acc: 0.570312]  [A loss: 0.763017, acc: 0.371094]\n",
      "519: [D loss: 0.684944, acc: 0.537109]  [A loss: 1.125578, acc: 0.015625]\n",
      "520: [D loss: 0.656931, acc: 0.621094]  [A loss: 0.723786, acc: 0.441406]\n",
      "521: [D loss: 0.689274, acc: 0.541016]  [A loss: 1.142721, acc: 0.011719]\n",
      "522: [D loss: 0.664074, acc: 0.599609]  [A loss: 0.673203, acc: 0.570312]\n",
      "523: [D loss: 0.709665, acc: 0.515625]  [A loss: 1.062880, acc: 0.023438]\n",
      "524: [D loss: 0.657242, acc: 0.625000]  [A loss: 0.756260, acc: 0.382812]\n",
      "525: [D loss: 0.674199, acc: 0.548828]  [A loss: 1.046920, acc: 0.054688]\n",
      "526: [D loss: 0.656044, acc: 0.601562]  [A loss: 0.771191, acc: 0.347656]\n",
      "527: [D loss: 0.674901, acc: 0.558594]  [A loss: 0.991757, acc: 0.089844]\n",
      "528: [D loss: 0.675202, acc: 0.582031]  [A loss: 0.873498, acc: 0.179688]\n",
      "529: [D loss: 0.684123, acc: 0.582031]  [A loss: 0.991338, acc: 0.058594]\n",
      "530: [D loss: 0.653870, acc: 0.628906]  [A loss: 0.824746, acc: 0.226562]\n",
      "531: [D loss: 0.675865, acc: 0.564453]  [A loss: 1.044064, acc: 0.031250]\n",
      "532: [D loss: 0.655970, acc: 0.607422]  [A loss: 0.769922, acc: 0.335938]\n",
      "533: [D loss: 0.709982, acc: 0.509766]  [A loss: 1.023315, acc: 0.042969]\n",
      "534: [D loss: 0.654647, acc: 0.638672]  [A loss: 0.732956, acc: 0.425781]\n",
      "535: [D loss: 0.723057, acc: 0.535156]  [A loss: 1.151036, acc: 0.031250]\n",
      "536: [D loss: 0.663512, acc: 0.601562]  [A loss: 0.717730, acc: 0.437500]\n",
      "537: [D loss: 0.695942, acc: 0.533203]  [A loss: 1.042010, acc: 0.042969]\n",
      "538: [D loss: 0.657933, acc: 0.617188]  [A loss: 0.745137, acc: 0.406250]\n",
      "539: [D loss: 0.686567, acc: 0.537109]  [A loss: 0.975309, acc: 0.070312]\n",
      "540: [D loss: 0.665812, acc: 0.578125]  [A loss: 0.935069, acc: 0.082031]\n",
      "541: [D loss: 0.669104, acc: 0.597656]  [A loss: 0.919587, acc: 0.152344]\n",
      "542: [D loss: 0.677529, acc: 0.589844]  [A loss: 0.886586, acc: 0.164062]\n",
      "543: [D loss: 0.677508, acc: 0.570312]  [A loss: 0.971699, acc: 0.074219]\n",
      "544: [D loss: 0.669970, acc: 0.582031]  [A loss: 0.824667, acc: 0.250000]\n",
      "545: [D loss: 0.674144, acc: 0.544922]  [A loss: 0.999930, acc: 0.070312]\n",
      "546: [D loss: 0.661614, acc: 0.617188]  [A loss: 0.802455, acc: 0.230469]\n",
      "547: [D loss: 0.685653, acc: 0.523438]  [A loss: 1.030943, acc: 0.039062]\n",
      "548: [D loss: 0.667939, acc: 0.597656]  [A loss: 0.816825, acc: 0.246094]\n",
      "549: [D loss: 0.686204, acc: 0.546875]  [A loss: 1.058478, acc: 0.062500]\n",
      "550: [D loss: 0.651581, acc: 0.648438]  [A loss: 0.730491, acc: 0.457031]\n",
      "551: [D loss: 0.702479, acc: 0.523438]  [A loss: 1.111603, acc: 0.035156]\n",
      "552: [D loss: 0.651953, acc: 0.626953]  [A loss: 0.755840, acc: 0.382812]\n",
      "553: [D loss: 0.679186, acc: 0.550781]  [A loss: 1.048112, acc: 0.062500]\n",
      "554: [D loss: 0.660485, acc: 0.603516]  [A loss: 0.752531, acc: 0.371094]\n",
      "555: [D loss: 0.693233, acc: 0.537109]  [A loss: 1.064839, acc: 0.015625]\n",
      "556: [D loss: 0.674701, acc: 0.582031]  [A loss: 0.769055, acc: 0.359375]\n",
      "557: [D loss: 0.686003, acc: 0.558594]  [A loss: 1.038646, acc: 0.062500]\n",
      "558: [D loss: 0.669796, acc: 0.605469]  [A loss: 0.763299, acc: 0.371094]\n",
      "559: [D loss: 0.696228, acc: 0.531250]  [A loss: 1.072616, acc: 0.019531]\n",
      "560: [D loss: 0.663440, acc: 0.585938]  [A loss: 0.687200, acc: 0.503906]\n",
      "561: [D loss: 0.709240, acc: 0.527344]  [A loss: 1.070080, acc: 0.027344]\n",
      "562: [D loss: 0.657466, acc: 0.621094]  [A loss: 0.807193, acc: 0.281250]\n",
      "563: [D loss: 0.686711, acc: 0.556641]  [A loss: 0.929051, acc: 0.113281]\n",
      "564: [D loss: 0.650484, acc: 0.630859]  [A loss: 0.855588, acc: 0.218750]\n",
      "565: [D loss: 0.690684, acc: 0.560547]  [A loss: 0.978738, acc: 0.070312]\n",
      "566: [D loss: 0.664044, acc: 0.615234]  [A loss: 0.847260, acc: 0.210938]\n",
      "567: [D loss: 0.685857, acc: 0.562500]  [A loss: 1.014709, acc: 0.058594]\n",
      "568: [D loss: 0.659353, acc: 0.619141]  [A loss: 0.831125, acc: 0.230469]\n",
      "569: [D loss: 0.697128, acc: 0.554688]  [A loss: 1.023644, acc: 0.046875]\n",
      "570: [D loss: 0.672375, acc: 0.578125]  [A loss: 0.829272, acc: 0.195312]\n",
      "571: [D loss: 0.681118, acc: 0.583984]  [A loss: 1.002338, acc: 0.050781]\n",
      "572: [D loss: 0.682719, acc: 0.556641]  [A loss: 0.913775, acc: 0.125000]\n",
      "573: [D loss: 0.668926, acc: 0.587891]  [A loss: 0.877406, acc: 0.187500]\n",
      "574: [D loss: 0.661947, acc: 0.576172]  [A loss: 0.986673, acc: 0.085938]\n",
      "575: [D loss: 0.659422, acc: 0.599609]  [A loss: 0.867578, acc: 0.230469]\n",
      "576: [D loss: 0.681782, acc: 0.556641]  [A loss: 1.078961, acc: 0.027344]\n",
      "577: [D loss: 0.659494, acc: 0.613281]  [A loss: 0.763459, acc: 0.355469]\n",
      "578: [D loss: 0.684267, acc: 0.546875]  [A loss: 1.124532, acc: 0.046875]\n",
      "579: [D loss: 0.673577, acc: 0.589844]  [A loss: 0.680825, acc: 0.546875]\n",
      "580: [D loss: 0.740102, acc: 0.517578]  [A loss: 1.143077, acc: 0.015625]\n",
      "581: [D loss: 0.684483, acc: 0.544922]  [A loss: 0.678388, acc: 0.562500]\n",
      "582: [D loss: 0.696110, acc: 0.541016]  [A loss: 0.989276, acc: 0.042969]\n",
      "583: [D loss: 0.666807, acc: 0.601562]  [A loss: 0.793716, acc: 0.296875]\n",
      "584: [D loss: 0.684752, acc: 0.529297]  [A loss: 0.947489, acc: 0.101562]\n",
      "585: [D loss: 0.666049, acc: 0.597656]  [A loss: 0.829798, acc: 0.269531]\n",
      "586: [D loss: 0.666694, acc: 0.583984]  [A loss: 0.971040, acc: 0.078125]\n",
      "587: [D loss: 0.672150, acc: 0.570312]  [A loss: 0.933174, acc: 0.152344]\n",
      "588: [D loss: 0.671890, acc: 0.558594]  [A loss: 0.892771, acc: 0.167969]\n",
      "589: [D loss: 0.674436, acc: 0.574219]  [A loss: 0.925799, acc: 0.117188]\n",
      "590: [D loss: 0.679116, acc: 0.562500]  [A loss: 0.934532, acc: 0.125000]\n",
      "591: [D loss: 0.667551, acc: 0.574219]  [A loss: 0.908914, acc: 0.164062]\n",
      "592: [D loss: 0.674810, acc: 0.574219]  [A loss: 0.971292, acc: 0.054688]\n",
      "593: [D loss: 0.670082, acc: 0.583984]  [A loss: 0.914249, acc: 0.175781]\n",
      "594: [D loss: 0.678177, acc: 0.550781]  [A loss: 0.967694, acc: 0.089844]\n",
      "595: [D loss: 0.673162, acc: 0.560547]  [A loss: 0.888605, acc: 0.136719]\n",
      "596: [D loss: 0.671305, acc: 0.585938]  [A loss: 0.909896, acc: 0.156250]\n",
      "597: [D loss: 0.666075, acc: 0.593750]  [A loss: 0.953816, acc: 0.109375]\n",
      "598: [D loss: 0.665284, acc: 0.587891]  [A loss: 0.954853, acc: 0.121094]\n",
      "599: [D loss: 0.665215, acc: 0.601562]  [A loss: 0.983597, acc: 0.109375]\n",
      "600: [D loss: 0.675232, acc: 0.564453]  [A loss: 0.872781, acc: 0.187500]\n",
      "601: [D loss: 0.687576, acc: 0.556641]  [A loss: 1.126875, acc: 0.035156]\n",
      "602: [D loss: 0.658375, acc: 0.595703]  [A loss: 0.684665, acc: 0.550781]\n",
      "603: [D loss: 0.715478, acc: 0.544922]  [A loss: 1.271842, acc: 0.003906]\n",
      "604: [D loss: 0.669447, acc: 0.583984]  [A loss: 0.591848, acc: 0.750000]\n",
      "605: [D loss: 0.765550, acc: 0.521484]  [A loss: 1.178579, acc: 0.007812]\n",
      "606: [D loss: 0.667870, acc: 0.585938]  [A loss: 0.715618, acc: 0.453125]\n",
      "607: [D loss: 0.684635, acc: 0.550781]  [A loss: 0.923781, acc: 0.113281]\n",
      "608: [D loss: 0.665966, acc: 0.611328]  [A loss: 0.853752, acc: 0.160156]\n",
      "609: [D loss: 0.673301, acc: 0.556641]  [A loss: 0.937950, acc: 0.089844]\n",
      "610: [D loss: 0.671234, acc: 0.566406]  [A loss: 0.853516, acc: 0.226562]\n",
      "611: [D loss: 0.681379, acc: 0.546875]  [A loss: 0.930793, acc: 0.105469]\n",
      "612: [D loss: 0.671746, acc: 0.589844]  [A loss: 0.882547, acc: 0.195312]\n",
      "613: [D loss: 0.669175, acc: 0.599609]  [A loss: 0.922895, acc: 0.113281]\n",
      "614: [D loss: 0.667988, acc: 0.593750]  [A loss: 0.907069, acc: 0.156250]\n",
      "615: [D loss: 0.685856, acc: 0.544922]  [A loss: 0.934055, acc: 0.128906]\n",
      "616: [D loss: 0.665520, acc: 0.591797]  [A loss: 0.815921, acc: 0.269531]\n",
      "617: [D loss: 0.692425, acc: 0.541016]  [A loss: 1.065828, acc: 0.042969]\n",
      "618: [D loss: 0.667732, acc: 0.593750]  [A loss: 0.752886, acc: 0.367188]\n",
      "619: [D loss: 0.692234, acc: 0.541016]  [A loss: 1.079249, acc: 0.050781]\n",
      "620: [D loss: 0.679404, acc: 0.554688]  [A loss: 0.777652, acc: 0.355469]\n",
      "621: [D loss: 0.688456, acc: 0.574219]  [A loss: 1.063257, acc: 0.027344]\n",
      "622: [D loss: 0.652985, acc: 0.630859]  [A loss: 0.726788, acc: 0.460938]\n",
      "623: [D loss: 0.694664, acc: 0.544922]  [A loss: 1.005614, acc: 0.062500]\n",
      "624: [D loss: 0.653130, acc: 0.613281]  [A loss: 0.860751, acc: 0.171875]\n",
      "625: [D loss: 0.684420, acc: 0.578125]  [A loss: 1.004459, acc: 0.089844]\n",
      "626: [D loss: 0.688594, acc: 0.552734]  [A loss: 0.876608, acc: 0.164062]\n",
      "627: [D loss: 0.682379, acc: 0.585938]  [A loss: 0.903991, acc: 0.136719]\n",
      "628: [D loss: 0.671560, acc: 0.597656]  [A loss: 0.903659, acc: 0.117188]\n",
      "629: [D loss: 0.655179, acc: 0.609375]  [A loss: 0.984805, acc: 0.078125]\n",
      "630: [D loss: 0.670454, acc: 0.607422]  [A loss: 0.838397, acc: 0.203125]\n",
      "631: [D loss: 0.676375, acc: 0.574219]  [A loss: 1.079079, acc: 0.046875]\n",
      "632: [D loss: 0.656418, acc: 0.607422]  [A loss: 0.783418, acc: 0.367188]\n",
      "633: [D loss: 0.679900, acc: 0.560547]  [A loss: 1.108585, acc: 0.019531]\n",
      "634: [D loss: 0.648273, acc: 0.640625]  [A loss: 0.748144, acc: 0.355469]\n",
      "635: [D loss: 0.700643, acc: 0.558594]  [A loss: 1.140427, acc: 0.019531]\n",
      "636: [D loss: 0.653458, acc: 0.615234]  [A loss: 0.699507, acc: 0.492188]\n",
      "637: [D loss: 0.702129, acc: 0.546875]  [A loss: 1.077209, acc: 0.050781]\n",
      "638: [D loss: 0.660694, acc: 0.603516]  [A loss: 0.728948, acc: 0.437500]\n",
      "639: [D loss: 0.689359, acc: 0.542969]  [A loss: 1.047857, acc: 0.062500]\n",
      "640: [D loss: 0.660118, acc: 0.599609]  [A loss: 0.792575, acc: 0.292969]\n",
      "641: [D loss: 0.684185, acc: 0.548828]  [A loss: 1.041114, acc: 0.039062]\n",
      "642: [D loss: 0.678623, acc: 0.552734]  [A loss: 0.796944, acc: 0.292969]\n",
      "643: [D loss: 0.698304, acc: 0.550781]  [A loss: 0.922117, acc: 0.175781]\n",
      "644: [D loss: 0.672144, acc: 0.566406]  [A loss: 0.873689, acc: 0.167969]\n",
      "645: [D loss: 0.667577, acc: 0.583984]  [A loss: 1.011138, acc: 0.058594]\n",
      "646: [D loss: 0.656788, acc: 0.591797]  [A loss: 0.757267, acc: 0.410156]\n",
      "647: [D loss: 0.709528, acc: 0.535156]  [A loss: 1.083265, acc: 0.031250]\n",
      "648: [D loss: 0.671122, acc: 0.591797]  [A loss: 0.713869, acc: 0.468750]\n",
      "649: [D loss: 0.702479, acc: 0.529297]  [A loss: 1.019866, acc: 0.074219]\n",
      "650: [D loss: 0.665649, acc: 0.595703]  [A loss: 0.775247, acc: 0.335938]\n",
      "651: [D loss: 0.692352, acc: 0.533203]  [A loss: 1.005377, acc: 0.058594]\n",
      "652: [D loss: 0.680675, acc: 0.562500]  [A loss: 0.801487, acc: 0.289062]\n",
      "653: [D loss: 0.669790, acc: 0.568359]  [A loss: 1.000289, acc: 0.070312]\n",
      "654: [D loss: 0.669663, acc: 0.578125]  [A loss: 0.815913, acc: 0.277344]\n",
      "655: [D loss: 0.675526, acc: 0.576172]  [A loss: 0.951495, acc: 0.085938]\n",
      "656: [D loss: 0.664646, acc: 0.607422]  [A loss: 0.895932, acc: 0.152344]\n",
      "657: [D loss: 0.680498, acc: 0.572266]  [A loss: 0.922262, acc: 0.089844]\n",
      "658: [D loss: 0.678992, acc: 0.576172]  [A loss: 0.896299, acc: 0.140625]\n",
      "659: [D loss: 0.669463, acc: 0.556641]  [A loss: 0.977298, acc: 0.093750]\n",
      "660: [D loss: 0.671901, acc: 0.574219]  [A loss: 0.809717, acc: 0.285156]\n",
      "661: [D loss: 0.667511, acc: 0.597656]  [A loss: 0.974875, acc: 0.113281]\n",
      "662: [D loss: 0.673160, acc: 0.572266]  [A loss: 0.863211, acc: 0.183594]\n",
      "663: [D loss: 0.662525, acc: 0.593750]  [A loss: 1.021515, acc: 0.101562]\n",
      "664: [D loss: 0.659730, acc: 0.605469]  [A loss: 0.846233, acc: 0.265625]\n",
      "665: [D loss: 0.688437, acc: 0.546875]  [A loss: 1.137758, acc: 0.011719]\n",
      "666: [D loss: 0.666187, acc: 0.572266]  [A loss: 0.710791, acc: 0.468750]\n",
      "667: [D loss: 0.718973, acc: 0.539062]  [A loss: 1.148817, acc: 0.019531]\n",
      "668: [D loss: 0.686267, acc: 0.539062]  [A loss: 0.703761, acc: 0.507812]\n",
      "669: [D loss: 0.701991, acc: 0.548828]  [A loss: 1.080416, acc: 0.015625]\n",
      "670: [D loss: 0.668004, acc: 0.609375]  [A loss: 0.737062, acc: 0.410156]\n",
      "671: [D loss: 0.711361, acc: 0.517578]  [A loss: 1.061390, acc: 0.035156]\n",
      "672: [D loss: 0.666633, acc: 0.621094]  [A loss: 0.789642, acc: 0.300781]\n",
      "673: [D loss: 0.673353, acc: 0.585938]  [A loss: 0.926658, acc: 0.121094]\n",
      "674: [D loss: 0.679759, acc: 0.576172]  [A loss: 0.869099, acc: 0.210938]\n",
      "675: [D loss: 0.661814, acc: 0.585938]  [A loss: 0.931712, acc: 0.132812]\n",
      "676: [D loss: 0.676083, acc: 0.617188]  [A loss: 0.920932, acc: 0.160156]\n",
      "677: [D loss: 0.669150, acc: 0.583984]  [A loss: 0.999590, acc: 0.066406]\n",
      "678: [D loss: 0.646206, acc: 0.615234]  [A loss: 0.777257, acc: 0.300781]\n",
      "679: [D loss: 0.697110, acc: 0.542969]  [A loss: 1.070566, acc: 0.027344]\n",
      "680: [D loss: 0.660312, acc: 0.609375]  [A loss: 0.778592, acc: 0.335938]\n",
      "681: [D loss: 0.684838, acc: 0.560547]  [A loss: 1.034582, acc: 0.058594]\n",
      "682: [D loss: 0.664617, acc: 0.601562]  [A loss: 0.755677, acc: 0.332031]\n",
      "683: [D loss: 0.683845, acc: 0.550781]  [A loss: 1.059458, acc: 0.058594]\n",
      "684: [D loss: 0.677120, acc: 0.568359]  [A loss: 0.835285, acc: 0.222656]\n",
      "685: [D loss: 0.664384, acc: 0.587891]  [A loss: 1.025102, acc: 0.070312]\n",
      "686: [D loss: 0.658892, acc: 0.611328]  [A loss: 0.815060, acc: 0.281250]\n",
      "687: [D loss: 0.680250, acc: 0.554688]  [A loss: 1.060628, acc: 0.085938]\n",
      "688: [D loss: 0.689290, acc: 0.525391]  [A loss: 0.853204, acc: 0.230469]\n",
      "689: [D loss: 0.686095, acc: 0.570312]  [A loss: 0.849454, acc: 0.187500]\n",
      "690: [D loss: 0.684601, acc: 0.529297]  [A loss: 0.928969, acc: 0.085938]\n",
      "691: [D loss: 0.693936, acc: 0.544922]  [A loss: 1.038883, acc: 0.078125]\n",
      "692: [D loss: 0.656291, acc: 0.607422]  [A loss: 0.814284, acc: 0.285156]\n",
      "693: [D loss: 0.660798, acc: 0.570312]  [A loss: 1.016771, acc: 0.078125]\n",
      "694: [D loss: 0.654972, acc: 0.609375]  [A loss: 0.797771, acc: 0.304688]\n",
      "695: [D loss: 0.675869, acc: 0.568359]  [A loss: 1.084463, acc: 0.062500]\n",
      "696: [D loss: 0.674983, acc: 0.574219]  [A loss: 0.749967, acc: 0.414062]\n",
      "697: [D loss: 0.710545, acc: 0.552734]  [A loss: 1.150171, acc: 0.023438]\n",
      "698: [D loss: 0.677471, acc: 0.585938]  [A loss: 0.726120, acc: 0.417969]\n",
      "699: [D loss: 0.681319, acc: 0.566406]  [A loss: 1.056012, acc: 0.050781]\n",
      "700: [D loss: 0.669247, acc: 0.583984]  [A loss: 0.782072, acc: 0.312500]\n",
      "701: [D loss: 0.672367, acc: 0.582031]  [A loss: 0.984181, acc: 0.085938]\n",
      "702: [D loss: 0.669909, acc: 0.613281]  [A loss: 0.792386, acc: 0.296875]\n",
      "703: [D loss: 0.676616, acc: 0.587891]  [A loss: 0.972922, acc: 0.093750]\n",
      "704: [D loss: 0.662633, acc: 0.589844]  [A loss: 0.800029, acc: 0.296875]\n",
      "705: [D loss: 0.697184, acc: 0.554688]  [A loss: 1.066849, acc: 0.042969]\n",
      "706: [D loss: 0.669669, acc: 0.576172]  [A loss: 0.794402, acc: 0.296875]\n",
      "707: [D loss: 0.700445, acc: 0.554688]  [A loss: 1.052357, acc: 0.035156]\n",
      "708: [D loss: 0.658701, acc: 0.619141]  [A loss: 0.772990, acc: 0.363281]\n",
      "709: [D loss: 0.683501, acc: 0.544922]  [A loss: 1.002362, acc: 0.054688]\n",
      "710: [D loss: 0.661631, acc: 0.595703]  [A loss: 0.839639, acc: 0.207031]\n",
      "711: [D loss: 0.670948, acc: 0.566406]  [A loss: 0.921777, acc: 0.144531]\n",
      "712: [D loss: 0.677944, acc: 0.607422]  [A loss: 0.866177, acc: 0.218750]\n",
      "713: [D loss: 0.674572, acc: 0.572266]  [A loss: 0.991532, acc: 0.082031]\n",
      "714: [D loss: 0.662531, acc: 0.621094]  [A loss: 0.774222, acc: 0.386719]\n",
      "715: [D loss: 0.684738, acc: 0.570312]  [A loss: 1.076168, acc: 0.050781]\n",
      "716: [D loss: 0.669248, acc: 0.582031]  [A loss: 0.772296, acc: 0.355469]\n",
      "717: [D loss: 0.739540, acc: 0.521484]  [A loss: 1.076699, acc: 0.054688]\n",
      "718: [D loss: 0.691498, acc: 0.521484]  [A loss: 0.788459, acc: 0.296875]\n",
      "719: [D loss: 0.710010, acc: 0.501953]  [A loss: 0.992236, acc: 0.082031]\n",
      "720: [D loss: 0.659167, acc: 0.611328]  [A loss: 0.790894, acc: 0.292969]\n",
      "721: [D loss: 0.700644, acc: 0.560547]  [A loss: 0.982609, acc: 0.074219]\n",
      "722: [D loss: 0.658346, acc: 0.591797]  [A loss: 0.818980, acc: 0.289062]\n",
      "723: [D loss: 0.676686, acc: 0.558594]  [A loss: 0.978723, acc: 0.078125]\n",
      "724: [D loss: 0.673541, acc: 0.585938]  [A loss: 0.870889, acc: 0.179688]\n",
      "725: [D loss: 0.657416, acc: 0.595703]  [A loss: 0.893609, acc: 0.203125]\n",
      "726: [D loss: 0.662493, acc: 0.591797]  [A loss: 0.891646, acc: 0.222656]\n",
      "727: [D loss: 0.681816, acc: 0.541016]  [A loss: 0.884810, acc: 0.226562]\n",
      "728: [D loss: 0.668105, acc: 0.576172]  [A loss: 0.876714, acc: 0.191406]\n",
      "729: [D loss: 0.669755, acc: 0.601562]  [A loss: 0.980414, acc: 0.082031]\n",
      "730: [D loss: 0.666536, acc: 0.568359]  [A loss: 0.884627, acc: 0.210938]\n",
      "731: [D loss: 0.681137, acc: 0.570312]  [A loss: 0.964504, acc: 0.078125]\n",
      "732: [D loss: 0.658237, acc: 0.589844]  [A loss: 0.842769, acc: 0.250000]\n",
      "733: [D loss: 0.686321, acc: 0.556641]  [A loss: 1.049803, acc: 0.054688]\n",
      "734: [D loss: 0.661642, acc: 0.609375]  [A loss: 0.743187, acc: 0.433594]\n",
      "735: [D loss: 0.703945, acc: 0.556641]  [A loss: 1.161103, acc: 0.027344]\n",
      "736: [D loss: 0.663826, acc: 0.593750]  [A loss: 0.699172, acc: 0.535156]\n",
      "737: [D loss: 0.706958, acc: 0.525391]  [A loss: 1.054218, acc: 0.039062]\n",
      "738: [D loss: 0.684219, acc: 0.568359]  [A loss: 0.750888, acc: 0.371094]\n",
      "739: [D loss: 0.718092, acc: 0.527344]  [A loss: 1.081151, acc: 0.039062]\n",
      "740: [D loss: 0.661301, acc: 0.607422]  [A loss: 0.675132, acc: 0.558594]\n",
      "741: [D loss: 0.717525, acc: 0.560547]  [A loss: 1.069389, acc: 0.039062]\n",
      "742: [D loss: 0.671545, acc: 0.589844]  [A loss: 0.732880, acc: 0.441406]\n",
      "743: [D loss: 0.701366, acc: 0.531250]  [A loss: 0.929953, acc: 0.082031]\n",
      "744: [D loss: 0.677327, acc: 0.591797]  [A loss: 0.814528, acc: 0.257812]\n",
      "745: [D loss: 0.691193, acc: 0.546875]  [A loss: 0.908527, acc: 0.125000]\n",
      "746: [D loss: 0.671512, acc: 0.605469]  [A loss: 0.865070, acc: 0.207031]\n",
      "747: [D loss: 0.687091, acc: 0.570312]  [A loss: 0.917390, acc: 0.140625]\n",
      "748: [D loss: 0.675741, acc: 0.587891]  [A loss: 0.882978, acc: 0.179688]\n",
      "749: [D loss: 0.686201, acc: 0.542969]  [A loss: 0.877975, acc: 0.191406]\n",
      "750: [D loss: 0.669799, acc: 0.587891]  [A loss: 0.849548, acc: 0.191406]\n",
      "751: [D loss: 0.691493, acc: 0.544922]  [A loss: 0.980520, acc: 0.085938]\n",
      "752: [D loss: 0.655907, acc: 0.634766]  [A loss: 0.795101, acc: 0.277344]\n",
      "753: [D loss: 0.680707, acc: 0.572266]  [A loss: 0.981444, acc: 0.058594]\n",
      "754: [D loss: 0.663448, acc: 0.593750]  [A loss: 0.777472, acc: 0.359375]\n",
      "755: [D loss: 0.672733, acc: 0.583984]  [A loss: 1.028493, acc: 0.046875]\n",
      "756: [D loss: 0.664672, acc: 0.613281]  [A loss: 0.772057, acc: 0.351562]\n",
      "757: [D loss: 0.695624, acc: 0.539062]  [A loss: 1.061801, acc: 0.078125]\n",
      "758: [D loss: 0.671257, acc: 0.591797]  [A loss: 0.722763, acc: 0.457031]\n",
      "759: [D loss: 0.696735, acc: 0.541016]  [A loss: 1.015699, acc: 0.046875]\n",
      "760: [D loss: 0.664135, acc: 0.607422]  [A loss: 0.756481, acc: 0.406250]\n",
      "761: [D loss: 0.727126, acc: 0.531250]  [A loss: 1.054922, acc: 0.035156]\n",
      "762: [D loss: 0.658565, acc: 0.599609]  [A loss: 0.735227, acc: 0.433594]\n",
      "763: [D loss: 0.715045, acc: 0.507812]  [A loss: 1.054772, acc: 0.046875]\n",
      "764: [D loss: 0.664963, acc: 0.585938]  [A loss: 0.742715, acc: 0.425781]\n",
      "765: [D loss: 0.700260, acc: 0.550781]  [A loss: 0.920720, acc: 0.164062]\n",
      "766: [D loss: 0.670087, acc: 0.578125]  [A loss: 0.859862, acc: 0.203125]\n",
      "767: [D loss: 0.677880, acc: 0.554688]  [A loss: 0.880754, acc: 0.175781]\n",
      "768: [D loss: 0.671934, acc: 0.597656]  [A loss: 0.834514, acc: 0.218750]\n",
      "769: [D loss: 0.688151, acc: 0.568359]  [A loss: 0.941917, acc: 0.128906]\n",
      "770: [D loss: 0.668757, acc: 0.582031]  [A loss: 0.814744, acc: 0.246094]\n",
      "771: [D loss: 0.680346, acc: 0.552734]  [A loss: 1.016826, acc: 0.042969]\n",
      "772: [D loss: 0.663213, acc: 0.576172]  [A loss: 0.786389, acc: 0.339844]\n",
      "773: [D loss: 0.684363, acc: 0.570312]  [A loss: 0.989588, acc: 0.074219]\n",
      "774: [D loss: 0.668724, acc: 0.593750]  [A loss: 0.880258, acc: 0.183594]\n",
      "775: [D loss: 0.688164, acc: 0.566406]  [A loss: 0.890158, acc: 0.171875]\n",
      "776: [D loss: 0.676530, acc: 0.580078]  [A loss: 0.861565, acc: 0.199219]\n",
      "777: [D loss: 0.665366, acc: 0.591797]  [A loss: 0.906684, acc: 0.160156]\n",
      "778: [D loss: 0.672743, acc: 0.583984]  [A loss: 0.909534, acc: 0.148438]\n",
      "779: [D loss: 0.678916, acc: 0.576172]  [A loss: 0.950520, acc: 0.121094]\n",
      "780: [D loss: 0.683415, acc: 0.572266]  [A loss: 0.891916, acc: 0.144531]\n",
      "781: [D loss: 0.674086, acc: 0.576172]  [A loss: 0.887104, acc: 0.207031]\n",
      "782: [D loss: 0.674559, acc: 0.582031]  [A loss: 0.891873, acc: 0.195312]\n",
      "783: [D loss: 0.698764, acc: 0.566406]  [A loss: 1.026094, acc: 0.085938]\n",
      "784: [D loss: 0.682067, acc: 0.556641]  [A loss: 0.884160, acc: 0.179688]\n",
      "785: [D loss: 0.688879, acc: 0.558594]  [A loss: 0.940560, acc: 0.125000]\n",
      "786: [D loss: 0.655020, acc: 0.642578]  [A loss: 0.847979, acc: 0.226562]\n",
      "787: [D loss: 0.683937, acc: 0.570312]  [A loss: 1.002288, acc: 0.085938]\n",
      "788: [D loss: 0.669233, acc: 0.580078]  [A loss: 0.821050, acc: 0.281250]\n",
      "789: [D loss: 0.698677, acc: 0.533203]  [A loss: 1.077044, acc: 0.062500]\n",
      "790: [D loss: 0.675380, acc: 0.589844]  [A loss: 0.725602, acc: 0.457031]\n",
      "791: [D loss: 0.716524, acc: 0.517578]  [A loss: 1.140193, acc: 0.023438]\n",
      "792: [D loss: 0.695720, acc: 0.539062]  [A loss: 0.659930, acc: 0.585938]\n",
      "793: [D loss: 0.719546, acc: 0.515625]  [A loss: 1.055986, acc: 0.031250]\n",
      "794: [D loss: 0.669207, acc: 0.583984]  [A loss: 0.710062, acc: 0.480469]\n",
      "795: [D loss: 0.735750, acc: 0.507812]  [A loss: 1.009637, acc: 0.039062]\n",
      "796: [D loss: 0.669929, acc: 0.560547]  [A loss: 0.753723, acc: 0.414062]\n",
      "797: [D loss: 0.693456, acc: 0.541016]  [A loss: 0.950740, acc: 0.097656]\n",
      "798: [D loss: 0.679455, acc: 0.548828]  [A loss: 0.805272, acc: 0.265625]\n",
      "799: [D loss: 0.675687, acc: 0.578125]  [A loss: 0.854002, acc: 0.250000]\n",
      "800: [D loss: 0.692345, acc: 0.537109]  [A loss: 0.857998, acc: 0.214844]\n",
      "801: [D loss: 0.669129, acc: 0.587891]  [A loss: 0.906616, acc: 0.117188]\n",
      "802: [D loss: 0.676528, acc: 0.587891]  [A loss: 0.931792, acc: 0.132812]\n",
      "803: [D loss: 0.672025, acc: 0.607422]  [A loss: 0.843736, acc: 0.265625]\n",
      "804: [D loss: 0.687144, acc: 0.539062]  [A loss: 0.923885, acc: 0.121094]\n",
      "805: [D loss: 0.679811, acc: 0.574219]  [A loss: 0.864030, acc: 0.187500]\n",
      "806: [D loss: 0.697380, acc: 0.562500]  [A loss: 0.938191, acc: 0.132812]\n",
      "807: [D loss: 0.678787, acc: 0.554688]  [A loss: 0.794668, acc: 0.320312]\n",
      "808: [D loss: 0.664306, acc: 0.582031]  [A loss: 0.965947, acc: 0.097656]\n",
      "809: [D loss: 0.674951, acc: 0.572266]  [A loss: 0.951256, acc: 0.144531]\n",
      "810: [D loss: 0.665235, acc: 0.595703]  [A loss: 0.896058, acc: 0.156250]\n",
      "811: [D loss: 0.662816, acc: 0.574219]  [A loss: 0.957253, acc: 0.132812]\n",
      "812: [D loss: 0.691849, acc: 0.546875]  [A loss: 0.916936, acc: 0.125000]\n",
      "813: [D loss: 0.666929, acc: 0.595703]  [A loss: 0.871611, acc: 0.230469]\n",
      "814: [D loss: 0.665037, acc: 0.576172]  [A loss: 0.948015, acc: 0.101562]\n",
      "815: [D loss: 0.675223, acc: 0.562500]  [A loss: 0.918563, acc: 0.136719]\n",
      "816: [D loss: 0.657444, acc: 0.621094]  [A loss: 0.932909, acc: 0.144531]\n",
      "817: [D loss: 0.674030, acc: 0.562500]  [A loss: 0.951673, acc: 0.125000]\n",
      "818: [D loss: 0.691287, acc: 0.527344]  [A loss: 1.018753, acc: 0.070312]\n",
      "819: [D loss: 0.690280, acc: 0.539062]  [A loss: 0.807623, acc: 0.316406]\n",
      "820: [D loss: 0.684497, acc: 0.562500]  [A loss: 1.094755, acc: 0.019531]\n",
      "821: [D loss: 0.674684, acc: 0.570312]  [A loss: 0.716844, acc: 0.472656]\n",
      "822: [D loss: 0.695559, acc: 0.554688]  [A loss: 1.136518, acc: 0.031250]\n",
      "823: [D loss: 0.676601, acc: 0.568359]  [A loss: 0.646665, acc: 0.625000]\n",
      "824: [D loss: 0.732237, acc: 0.509766]  [A loss: 1.096228, acc: 0.023438]\n",
      "825: [D loss: 0.678945, acc: 0.566406]  [A loss: 0.788696, acc: 0.300781]\n",
      "826: [D loss: 0.686248, acc: 0.574219]  [A loss: 0.927381, acc: 0.128906]\n",
      "827: [D loss: 0.673472, acc: 0.578125]  [A loss: 0.780907, acc: 0.335938]\n",
      "828: [D loss: 0.681983, acc: 0.558594]  [A loss: 0.894298, acc: 0.128906]\n",
      "829: [D loss: 0.661828, acc: 0.623047]  [A loss: 0.821119, acc: 0.261719]\n",
      "830: [D loss: 0.694084, acc: 0.552734]  [A loss: 0.981083, acc: 0.074219]\n",
      "831: [D loss: 0.670724, acc: 0.568359]  [A loss: 0.800492, acc: 0.269531]\n",
      "832: [D loss: 0.666313, acc: 0.597656]  [A loss: 0.908435, acc: 0.132812]\n",
      "833: [D loss: 0.671539, acc: 0.589844]  [A loss: 0.881000, acc: 0.144531]\n",
      "834: [D loss: 0.667776, acc: 0.605469]  [A loss: 0.909677, acc: 0.171875]\n",
      "835: [D loss: 0.660465, acc: 0.595703]  [A loss: 0.889636, acc: 0.187500]\n",
      "836: [D loss: 0.681979, acc: 0.580078]  [A loss: 1.024733, acc: 0.054688]\n",
      "837: [D loss: 0.689571, acc: 0.533203]  [A loss: 0.859051, acc: 0.195312]\n",
      "838: [D loss: 0.677978, acc: 0.562500]  [A loss: 0.932779, acc: 0.121094]\n",
      "839: [D loss: 0.683496, acc: 0.583984]  [A loss: 0.858036, acc: 0.218750]\n",
      "840: [D loss: 0.689772, acc: 0.546875]  [A loss: 0.965251, acc: 0.082031]\n",
      "841: [D loss: 0.652550, acc: 0.611328]  [A loss: 0.804767, acc: 0.292969]\n",
      "842: [D loss: 0.684749, acc: 0.572266]  [A loss: 1.094681, acc: 0.039062]\n",
      "843: [D loss: 0.685803, acc: 0.546875]  [A loss: 0.677490, acc: 0.531250]\n",
      "844: [D loss: 0.707834, acc: 0.548828]  [A loss: 1.142203, acc: 0.015625]\n",
      "845: [D loss: 0.667786, acc: 0.593750]  [A loss: 0.679427, acc: 0.523438]\n",
      "846: [D loss: 0.729620, acc: 0.529297]  [A loss: 1.029304, acc: 0.070312]\n",
      "847: [D loss: 0.692302, acc: 0.546875]  [A loss: 0.801238, acc: 0.281250]\n",
      "848: [D loss: 0.687662, acc: 0.541016]  [A loss: 0.938751, acc: 0.089844]\n",
      "849: [D loss: 0.681684, acc: 0.550781]  [A loss: 0.854532, acc: 0.242188]\n",
      "850: [D loss: 0.703177, acc: 0.541016]  [A loss: 0.918523, acc: 0.132812]\n",
      "851: [D loss: 0.676842, acc: 0.560547]  [A loss: 0.800475, acc: 0.296875]\n",
      "852: [D loss: 0.679729, acc: 0.535156]  [A loss: 0.925338, acc: 0.097656]\n",
      "853: [D loss: 0.689808, acc: 0.539062]  [A loss: 0.862132, acc: 0.187500]\n",
      "854: [D loss: 0.684976, acc: 0.550781]  [A loss: 0.884933, acc: 0.164062]\n",
      "855: [D loss: 0.674407, acc: 0.587891]  [A loss: 0.838270, acc: 0.265625]\n",
      "856: [D loss: 0.674441, acc: 0.572266]  [A loss: 0.948566, acc: 0.101562]\n",
      "857: [D loss: 0.660256, acc: 0.630859]  [A loss: 0.809695, acc: 0.273438]\n",
      "858: [D loss: 0.692989, acc: 0.554688]  [A loss: 0.988921, acc: 0.085938]\n",
      "859: [D loss: 0.676239, acc: 0.583984]  [A loss: 0.809638, acc: 0.312500]\n",
      "860: [D loss: 0.683856, acc: 0.560547]  [A loss: 0.997825, acc: 0.089844]\n",
      "861: [D loss: 0.666832, acc: 0.572266]  [A loss: 0.738008, acc: 0.421875]\n",
      "862: [D loss: 0.709723, acc: 0.531250]  [A loss: 1.072810, acc: 0.023438]\n",
      "863: [D loss: 0.682536, acc: 0.566406]  [A loss: 0.712649, acc: 0.445312]\n",
      "864: [D loss: 0.690284, acc: 0.529297]  [A loss: 0.990850, acc: 0.074219]\n",
      "865: [D loss: 0.679957, acc: 0.568359]  [A loss: 0.714607, acc: 0.488281]\n",
      "866: [D loss: 0.708728, acc: 0.529297]  [A loss: 0.986912, acc: 0.085938]\n",
      "867: [D loss: 0.670979, acc: 0.599609]  [A loss: 0.751344, acc: 0.390625]\n",
      "868: [D loss: 0.713194, acc: 0.517578]  [A loss: 0.990776, acc: 0.039062]\n",
      "869: [D loss: 0.664167, acc: 0.593750]  [A loss: 0.737730, acc: 0.433594]\n",
      "870: [D loss: 0.694402, acc: 0.542969]  [A loss: 0.940678, acc: 0.066406]\n",
      "871: [D loss: 0.688838, acc: 0.560547]  [A loss: 0.816860, acc: 0.238281]\n",
      "872: [D loss: 0.657680, acc: 0.585938]  [A loss: 0.869295, acc: 0.175781]\n",
      "873: [D loss: 0.682483, acc: 0.570312]  [A loss: 0.864673, acc: 0.210938]\n",
      "874: [D loss: 0.669351, acc: 0.591797]  [A loss: 0.860556, acc: 0.199219]\n",
      "875: [D loss: 0.688009, acc: 0.537109]  [A loss: 0.890314, acc: 0.179688]\n",
      "876: [D loss: 0.675457, acc: 0.582031]  [A loss: 0.864913, acc: 0.203125]\n",
      "877: [D loss: 0.673164, acc: 0.599609]  [A loss: 0.798030, acc: 0.281250]\n",
      "878: [D loss: 0.694205, acc: 0.562500]  [A loss: 0.941812, acc: 0.109375]\n",
      "879: [D loss: 0.674625, acc: 0.576172]  [A loss: 0.747773, acc: 0.378906]\n",
      "880: [D loss: 0.700359, acc: 0.513672]  [A loss: 1.076060, acc: 0.011719]\n",
      "881: [D loss: 0.678580, acc: 0.570312]  [A loss: 0.689058, acc: 0.539062]\n",
      "882: [D loss: 0.726545, acc: 0.515625]  [A loss: 1.070670, acc: 0.011719]\n",
      "883: [D loss: 0.678747, acc: 0.560547]  [A loss: 0.707129, acc: 0.484375]\n",
      "884: [D loss: 0.690980, acc: 0.556641]  [A loss: 0.911072, acc: 0.128906]\n",
      "885: [D loss: 0.682126, acc: 0.568359]  [A loss: 0.813970, acc: 0.289062]\n",
      "886: [D loss: 0.695010, acc: 0.533203]  [A loss: 0.929870, acc: 0.125000]\n",
      "887: [D loss: 0.679879, acc: 0.564453]  [A loss: 0.811289, acc: 0.265625]\n",
      "888: [D loss: 0.669640, acc: 0.564453]  [A loss: 0.844227, acc: 0.218750]\n",
      "889: [D loss: 0.670962, acc: 0.589844]  [A loss: 0.849181, acc: 0.214844]\n",
      "890: [D loss: 0.681953, acc: 0.541016]  [A loss: 0.895014, acc: 0.121094]\n",
      "891: [D loss: 0.683995, acc: 0.558594]  [A loss: 0.871888, acc: 0.191406]\n",
      "892: [D loss: 0.684339, acc: 0.556641]  [A loss: 0.882452, acc: 0.171875]\n",
      "893: [D loss: 0.683062, acc: 0.560547]  [A loss: 0.902742, acc: 0.121094]\n",
      "894: [D loss: 0.668584, acc: 0.595703]  [A loss: 0.890089, acc: 0.210938]\n",
      "895: [D loss: 0.661370, acc: 0.585938]  [A loss: 0.887808, acc: 0.175781]\n",
      "896: [D loss: 0.672392, acc: 0.562500]  [A loss: 0.865716, acc: 0.222656]\n",
      "897: [D loss: 0.681141, acc: 0.552734]  [A loss: 0.928638, acc: 0.136719]\n",
      "898: [D loss: 0.673999, acc: 0.580078]  [A loss: 0.878233, acc: 0.175781]\n",
      "899: [D loss: 0.688535, acc: 0.521484]  [A loss: 1.008658, acc: 0.066406]\n",
      "900: [D loss: 0.664118, acc: 0.613281]  [A loss: 0.716505, acc: 0.464844]\n",
      "901: [D loss: 0.702977, acc: 0.556641]  [A loss: 1.085283, acc: 0.046875]\n",
      "902: [D loss: 0.675646, acc: 0.589844]  [A loss: 0.670950, acc: 0.570312]\n",
      "903: [D loss: 0.710717, acc: 0.519531]  [A loss: 1.001345, acc: 0.031250]\n",
      "904: [D loss: 0.692483, acc: 0.558594]  [A loss: 0.772080, acc: 0.335938]\n",
      "905: [D loss: 0.694555, acc: 0.533203]  [A loss: 0.953198, acc: 0.074219]\n",
      "906: [D loss: 0.681739, acc: 0.552734]  [A loss: 0.809624, acc: 0.265625]\n",
      "907: [D loss: 0.692887, acc: 0.531250]  [A loss: 0.964412, acc: 0.082031]\n",
      "908: [D loss: 0.667174, acc: 0.576172]  [A loss: 0.777887, acc: 0.296875]\n",
      "909: [D loss: 0.680198, acc: 0.566406]  [A loss: 0.990593, acc: 0.078125]\n",
      "910: [D loss: 0.656828, acc: 0.611328]  [A loss: 0.802122, acc: 0.273438]\n",
      "911: [D loss: 0.687397, acc: 0.566406]  [A loss: 0.964662, acc: 0.078125]\n",
      "912: [D loss: 0.704026, acc: 0.523438]  [A loss: 0.861659, acc: 0.187500]\n",
      "913: [D loss: 0.693241, acc: 0.521484]  [A loss: 0.898224, acc: 0.121094]\n",
      "914: [D loss: 0.703456, acc: 0.527344]  [A loss: 0.834533, acc: 0.203125]\n",
      "915: [D loss: 0.676743, acc: 0.587891]  [A loss: 0.849024, acc: 0.210938]\n",
      "916: [D loss: 0.676396, acc: 0.550781]  [A loss: 0.927202, acc: 0.105469]\n",
      "917: [D loss: 0.694869, acc: 0.517578]  [A loss: 0.883114, acc: 0.191406]\n",
      "918: [D loss: 0.669467, acc: 0.597656]  [A loss: 0.878530, acc: 0.195312]\n",
      "919: [D loss: 0.689582, acc: 0.535156]  [A loss: 0.866602, acc: 0.183594]\n",
      "920: [D loss: 0.681199, acc: 0.544922]  [A loss: 0.839738, acc: 0.222656]\n",
      "921: [D loss: 0.690775, acc: 0.531250]  [A loss: 1.004681, acc: 0.054688]\n",
      "922: [D loss: 0.675504, acc: 0.564453]  [A loss: 0.765092, acc: 0.390625]\n",
      "923: [D loss: 0.695599, acc: 0.521484]  [A loss: 1.085426, acc: 0.039062]\n",
      "924: [D loss: 0.688058, acc: 0.535156]  [A loss: 0.696001, acc: 0.511719]\n",
      "925: [D loss: 0.688695, acc: 0.556641]  [A loss: 0.986554, acc: 0.078125]\n",
      "926: [D loss: 0.673615, acc: 0.562500]  [A loss: 0.735398, acc: 0.410156]\n",
      "927: [D loss: 0.692980, acc: 0.560547]  [A loss: 0.984521, acc: 0.066406]\n",
      "928: [D loss: 0.666959, acc: 0.583984]  [A loss: 0.817257, acc: 0.234375]\n",
      "929: [D loss: 0.679852, acc: 0.550781]  [A loss: 0.919952, acc: 0.128906]\n",
      "930: [D loss: 0.678463, acc: 0.570312]  [A loss: 0.813728, acc: 0.292969]\n",
      "931: [D loss: 0.678411, acc: 0.556641]  [A loss: 0.972631, acc: 0.093750]\n",
      "932: [D loss: 0.669859, acc: 0.574219]  [A loss: 0.780738, acc: 0.339844]\n",
      "933: [D loss: 0.691859, acc: 0.537109]  [A loss: 0.974460, acc: 0.066406]\n",
      "934: [D loss: 0.675011, acc: 0.568359]  [A loss: 0.748389, acc: 0.394531]\n",
      "935: [D loss: 0.681932, acc: 0.564453]  [A loss: 1.003938, acc: 0.078125]\n",
      "936: [D loss: 0.655482, acc: 0.625000]  [A loss: 0.760146, acc: 0.378906]\n",
      "937: [D loss: 0.693468, acc: 0.535156]  [A loss: 0.926156, acc: 0.109375]\n",
      "938: [D loss: 0.664150, acc: 0.605469]  [A loss: 0.799906, acc: 0.320312]\n",
      "939: [D loss: 0.689463, acc: 0.531250]  [A loss: 0.986625, acc: 0.074219]\n",
      "940: [D loss: 0.678842, acc: 0.572266]  [A loss: 0.799552, acc: 0.269531]\n",
      "941: [D loss: 0.684360, acc: 0.521484]  [A loss: 0.895665, acc: 0.148438]\n",
      "942: [D loss: 0.672965, acc: 0.580078]  [A loss: 0.864069, acc: 0.148438]\n",
      "943: [D loss: 0.682367, acc: 0.568359]  [A loss: 0.919094, acc: 0.171875]\n",
      "944: [D loss: 0.678494, acc: 0.566406]  [A loss: 0.862503, acc: 0.195312]\n",
      "945: [D loss: 0.685596, acc: 0.562500]  [A loss: 0.881800, acc: 0.128906]\n",
      "946: [D loss: 0.676992, acc: 0.570312]  [A loss: 0.890334, acc: 0.156250]\n",
      "947: [D loss: 0.678550, acc: 0.560547]  [A loss: 0.881853, acc: 0.187500]\n",
      "948: [D loss: 0.664828, acc: 0.589844]  [A loss: 0.914809, acc: 0.171875]\n",
      "949: [D loss: 0.679986, acc: 0.580078]  [A loss: 0.904969, acc: 0.167969]\n",
      "950: [D loss: 0.666890, acc: 0.607422]  [A loss: 0.811196, acc: 0.312500]\n",
      "951: [D loss: 0.687905, acc: 0.535156]  [A loss: 1.019562, acc: 0.058594]\n",
      "952: [D loss: 0.683633, acc: 0.544922]  [A loss: 0.808829, acc: 0.332031]\n",
      "953: [D loss: 0.694233, acc: 0.562500]  [A loss: 1.095714, acc: 0.050781]\n",
      "954: [D loss: 0.674452, acc: 0.570312]  [A loss: 0.655300, acc: 0.621094]\n",
      "955: [D loss: 0.717276, acc: 0.531250]  [A loss: 1.141309, acc: 0.003906]\n",
      "956: [D loss: 0.675878, acc: 0.583984]  [A loss: 0.681761, acc: 0.562500]\n",
      "957: [D loss: 0.739994, acc: 0.523438]  [A loss: 0.953980, acc: 0.089844]\n",
      "958: [D loss: 0.676411, acc: 0.572266]  [A loss: 0.766825, acc: 0.332031]\n",
      "959: [D loss: 0.686803, acc: 0.562500]  [A loss: 0.913512, acc: 0.121094]\n",
      "960: [D loss: 0.675915, acc: 0.585938]  [A loss: 0.782987, acc: 0.324219]\n",
      "961: [D loss: 0.687437, acc: 0.574219]  [A loss: 0.892538, acc: 0.152344]\n",
      "962: [D loss: 0.673673, acc: 0.542969]  [A loss: 0.870397, acc: 0.203125]\n",
      "963: [D loss: 0.671660, acc: 0.578125]  [A loss: 0.850325, acc: 0.246094]\n",
      "964: [D loss: 0.674502, acc: 0.595703]  [A loss: 0.887812, acc: 0.144531]\n",
      "965: [D loss: 0.677027, acc: 0.564453]  [A loss: 0.883018, acc: 0.195312]\n",
      "966: [D loss: 0.704135, acc: 0.503906]  [A loss: 0.974802, acc: 0.093750]\n",
      "967: [D loss: 0.670059, acc: 0.582031]  [A loss: 0.793573, acc: 0.300781]\n",
      "968: [D loss: 0.678116, acc: 0.570312]  [A loss: 0.896132, acc: 0.144531]\n",
      "969: [D loss: 0.676410, acc: 0.564453]  [A loss: 0.836582, acc: 0.214844]\n",
      "970: [D loss: 0.693709, acc: 0.531250]  [A loss: 0.915422, acc: 0.152344]\n",
      "971: [D loss: 0.683532, acc: 0.570312]  [A loss: 0.833612, acc: 0.257812]\n",
      "972: [D loss: 0.675607, acc: 0.597656]  [A loss: 0.948975, acc: 0.152344]\n",
      "973: [D loss: 0.684984, acc: 0.556641]  [A loss: 0.818461, acc: 0.253906]\n",
      "974: [D loss: 0.681395, acc: 0.564453]  [A loss: 0.918987, acc: 0.160156]\n",
      "975: [D loss: 0.685495, acc: 0.566406]  [A loss: 0.824302, acc: 0.273438]\n",
      "976: [D loss: 0.686285, acc: 0.535156]  [A loss: 0.916208, acc: 0.117188]\n",
      "977: [D loss: 0.677088, acc: 0.564453]  [A loss: 0.859171, acc: 0.199219]\n",
      "978: [D loss: 0.678620, acc: 0.552734]  [A loss: 0.921079, acc: 0.128906]\n",
      "979: [D loss: 0.698774, acc: 0.507812]  [A loss: 0.861896, acc: 0.207031]\n",
      "980: [D loss: 0.679778, acc: 0.558594]  [A loss: 0.942994, acc: 0.093750]\n",
      "981: [D loss: 0.675318, acc: 0.578125]  [A loss: 0.763160, acc: 0.363281]\n",
      "982: [D loss: 0.694782, acc: 0.533203]  [A loss: 1.084848, acc: 0.042969]\n",
      "983: [D loss: 0.681042, acc: 0.562500]  [A loss: 0.669222, acc: 0.582031]\n",
      "984: [D loss: 0.737660, acc: 0.523438]  [A loss: 1.125993, acc: 0.027344]\n",
      "985: [D loss: 0.673135, acc: 0.568359]  [A loss: 0.681404, acc: 0.527344]\n",
      "986: [D loss: 0.707672, acc: 0.515625]  [A loss: 0.951747, acc: 0.089844]\n",
      "987: [D loss: 0.686300, acc: 0.560547]  [A loss: 0.719482, acc: 0.468750]\n",
      "988: [D loss: 0.715753, acc: 0.521484]  [A loss: 0.918598, acc: 0.097656]\n",
      "989: [D loss: 0.678120, acc: 0.566406]  [A loss: 0.826245, acc: 0.269531]\n",
      "990: [D loss: 0.698087, acc: 0.527344]  [A loss: 0.926121, acc: 0.078125]\n",
      "991: [D loss: 0.681642, acc: 0.583984]  [A loss: 0.758178, acc: 0.367188]\n",
      "992: [D loss: 0.701670, acc: 0.562500]  [A loss: 0.900117, acc: 0.156250]\n",
      "993: [D loss: 0.675008, acc: 0.617188]  [A loss: 0.848111, acc: 0.230469]\n",
      "994: [D loss: 0.701574, acc: 0.523438]  [A loss: 0.928084, acc: 0.097656]\n",
      "995: [D loss: 0.675648, acc: 0.582031]  [A loss: 0.778208, acc: 0.312500]\n",
      "996: [D loss: 0.701494, acc: 0.539062]  [A loss: 0.912037, acc: 0.156250]\n",
      "997: [D loss: 0.675204, acc: 0.587891]  [A loss: 0.802925, acc: 0.304688]\n",
      "998: [D loss: 0.687939, acc: 0.527344]  [A loss: 0.916867, acc: 0.128906]\n",
      "999: [D loss: 0.674546, acc: 0.570312]  [A loss: 0.811018, acc: 0.277344]\n",
      "time: 8h 27min\n"
     ]
    }
   ],
   "source": [
    "model.train(train_steps=1000, batch_size=256, save_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 925 ms\n"
     ]
    }
   ],
   "source": [
    "model.plot_images(fake=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 532 ms\n"
     ]
    }
   ],
   "source": [
    "model.plot_images(fake=False, save2file=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
